<center>

![alt text](image.png)
# <font face="黑体">《程序设计实验》概要设计文档
![alt text](image-1.png)
<br>

### 题目：<u>____"AI灵宠"虚拟桌宠开发_____</u></font>

<font face="等线"><font size="5"><b> 
班&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;级&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2024219105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2024219104
姓&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;名&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乐长昕&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;罗添元&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
学&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;号&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2023212455&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2023212823

指导教师&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;吴晓非&nbsp;&nbsp;&nbsp;&nbsp;
</font></font>
<font face="Times New Roman"><font size="5">2025 年 11 月</font></font>
<br><br>

</center>
</b>

## 1、系统概述
### 1.1、系统简介
&emsp;&emsp;本项目是一款运行在桌面环境下的虚拟伙伴系统，结合了 **Python、Live2D 动画、语音识别（ASR）、自然语言生成（LLM）、语音合成（TTS）** 等技术，实现一个能够陪伴用户、与用户对话并做出自然动作反馈的智能桌宠。系统通过图形界面展示角色，通过麦克风和扬声器与用户沟通，并在内部通过逻辑控制模块协调对话、表情、动作与语音输出等过程。
&emsp;&emsp;项目的核心目标是**自然交互**。桌宠不仅能回答问题，更能通过表情、动作与语音表现情绪，使交互具有拟人化特点。Live2D 角色模型支持多种动作绑定，例如点头、眨眼、摇头、微笑等，而系统的对话生成也会结合这些动作丰富交互体验。
&emsp;&emsp;在文档中将会出现 LLM、ASR、TTS 等术语，分别表示语言模型（Large Language Model）、自动语音识别（Automatic Speech Recognition）和语音合成（Text To Speech）。

### 1.2、系统目标
&emsp;&emsp;系统开发的目标主要集中在可用性、自然交互性和可扩展性三个方面。

**功能目标与可用性：**
- 支持 Live2D 模型展示与动作播放；
- 支持语音识别，允许用户通过麦克风与角色沟通；
- 支持自然语言理解并生成语音回复；
- 支持角色表情、动作与语言同步；
- 提供桌面常驻模式；
- 支持基本脚本化或插件式扩展，例如自定义指令、快捷命令、动作触发等；
  
**性能目标与自然交互性：**
- Live2D 模型渲染流畅稳定，不出现卡顿；
- 对话响应延迟尽量控制在可接受范围；
- 语音识别和合成在低配置设备上仍可顺利运行；
- 系统逻辑解耦，便于未来新增新角色或新动作；

**可拓展性：**
- 支持引入本地训练的语言&语音模型接口，支持运行用户自行训练、本地推理的模型；


### 1.3、系统运行环境
&emsp;&emsp;为了保证 Live2D 渲染效果和语音处理性能，本项目推荐运行环境如下：

**硬件平台：** 普通家用或办公电脑即可，建议 8GB 内存以上
**操作系统：** Windows 10/11（优先）；Linux 支持 Web 端功能；
**数据库系统：** 无需传统数据库，采用 JSON 文件存储；
**编程平台：** Python 3.10；
**网络协议：** HTTP（用于 Web 本地服务和调用外部 API）；
**依赖环境：** PyQt5、Live2D SDK、FastAPI、OpenCV、语音处理模型依赖等；
### 1.4、开发环境 
&emsp;&emsp;项目的开发所需工具如下：
**开发语言：** Python 3.10
**开发工具：** Visual Studio Code / PyCharm
**版本管理：** Git / Github
**主要依赖库：**
- PyQt5（界面）
- Live2D Cubism SDK（角色渲染）
- OpenCV（图像处理）
- FastAPI（接口扩展）
- pyaudio / sounddevice（音频输入输出）
- requests（网络通信）
- 每个模块的依赖已在 requirements.txt 中列出
### 1.5、关键知识点
<small>(右侧为格式bug 后无内容)</small>
|序号|知识点1|关系|知识点2||
|-|-|-|-|-|
|1|Python 编程|支撑|界面开发（PyQt5）||
|2|Python 编程|支撑|后端逻辑控制||
|3|PyQt5|驱动|Live2D 模型展示||
|4|Live2D 动画|关联|动作系统||
|5|ASR（语音识别）|输入|对话系统||
|6|LLM（语言模型）|决策|动作触发系统||
|7|LLM|输出|文本生成||
|8|TTS（语音合成）|输出语音|音频播放系统||
|9|JSON 配置|存储|动作绑定/历史记录||


### 1.6、知识链条
&emsp;&emsp;通过本项目，各知识点之间构成如下链条：
- **Python 编程（1）** → **PyQt5 图形界面（3）** → **Live2D 动画展示（4）** 
-  **ASR（5）** → **LLM 输入与理解（6）** → **文本生成（7）** → **TTS 输出（8）**
-  **LLM 输出（6）** → **动作系统（4）** → **动作事件调度（9）**

&emsp;&emsp;整体形成**语音输入 → 理解 → 动作/语音输出**的交互链。
## 2、总体结构设计
### 2.1、软件结构
&emsp;&emsp;本系统采用分层结构设计，主要划分为以下子系统：
- **界面层（UI Layer）：** 负责 Live2D 模型呈现、PyQt 窗口管理、动作播放等界面逻辑。
- **逻辑控制层（Logic Layer）：** 管理事件队列、对话流程、动作调度以及系统运行状态，是整个系统的核心。
- **智能处理层（AI Layer）：** 包含语音识别、语言模型、语音合成等子模块。
- **数据层（Data Layer）：** 管理 JSON 配置文件、角色动作绑定、对话历史等。
  
&emsp;&emsp;模块之间呈现松散耦合关系，通过事件流和接口进行交互，不直接互相依赖。
### 2.2、设计思想
&emsp;&emsp;系统采取**松耦合、模块化**的设计方式，使得每个智能模块（ASR/LLM/TTS）可随时替换和升级。同时，界面层不直接处理 AI 模块的逻辑，而由逻辑控制层统一调度，从而保持角色表现一致性。
&emsp;&emsp;整个项目的设计理念是让用户的每一次输入都能经过完整链路处理：
&emsp;&emsp;语音 → 文本 → 理解 → 生成 → 行为执行 → 语音回复 → 动作反馈。
&emsp;&emsp;这样可以实现自然互动效果，同时便于未来扩展，例如添加“提醒事项、记账插件、命令系统等”。
## 3、模块设计
### 3.1 Live2D 展示模块
#### 3.1.1 功能描述
&emsp;&emsp;该模块负责**角色模型加载、渲染以及动作播放**。用户看到的所有表情、动作、状态切换均通过此模块完成。
#### 3.1.2 接口描述
- **输入：** 动作名称、动作参数、表情指令。
- **输出：** 渲染画面、动作动画。
- **异常处理：** 模型文件缺失、动作不存在时给出默认动作。
#### 3.1.3 数据结构描述
&emsp;&emsp;使用**JSON 存储动作表** ，例如：
~~~
{
  "smile": "motion_smile.mtn",
  "wave": "motion_wave.mtn"
}
~~~
#### 3.1.4 实现思路
&emsp;&emsp;通过 **PyQt5** 绘制事件绑定 **Live2D SDK** 渲染方法；动作调用通过一个事件队列保证播放次序稳定。
### 3.2 对话管理模块（核心模块）
#### 3.2.1 功能描述
&emsp;&emsp;负责管理从 **ASR** 输入到 **LLM** 输出再到**动作触发**的整个流程。
#### 3.2.2 接口描述
- **输入：** 文本输入或语音识别结果
- **输出：** 文本回复、触发动作、调用 TTS
#### 3.2.3 数据结构描述
&emsp;&emsp;维护上下文对话列表：
~~~
[
  {"role": "user", "content": "你好"},
  {"role": "assistant", "content": "你好呀，我在呢"}
]
~~~
#### 3.2.4 实现思路
&emsp;&emsp;根据用户输入触发 LLM 请求 → 分析情绪与动作标签 → 分发给 Live2D 和 TTS 模块 → 控制整个交互节奏
## 4、数据库与数据结构设计 
### 4.1、 数据库及数据表 
&emsp;&emsp;（系统不以数据库方式存储数据）
### 4.2、 数据结构设计 
&emsp;&emsp;**主要数据结构**包括：
- config.json（全局配置）
- motions.json（角色动作表）
- dialog_history.json（历史对话）

&emsp;&emsp;每个文件采用键值对结构，便于修改和加载。
### 4.3、 数据存储设计 
&emsp;&emsp;JSON 文件存储在项目 data 目录中，通过普通文件 I/O 访问，避免外部依赖。
## 5、接口设计 
### 5.1、外部接口
&emsp;&emsp;系统内部提供 FastAPI 服务，外部程序可向接口发送文本或动作指令，实现跨设备控制桌宠。
### 5.2、内部接口
&emsp;&emsp;内部模块通过函数调用与事件调度协同工作：
- ASR & LLM → 对话管理器
- 对话管理器 → LLM & TTS
## 6、其他设计 
&emsp;&emsp;系统额外设计了如下内容：
- 异常恢复机制（如网络错误自动重试）
- 日志系统
- 动作优先级系统
- 配置热加载（无需重启即可修改角色动作）