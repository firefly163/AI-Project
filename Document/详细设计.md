<center>

![alt text](image.png)
# <font face="黑体">《程序设计实验》详细设计文档
![alt text](image-1.png)
<br>

### 题目：<u>“AI灵宠”：面向陪伴交互的多模型虚拟桌宠系统开发</u></font>

<font face="等线"><font size="5"><b> 
班&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;级&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2024219105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2024219104
姓&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;名&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乐长昕&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;罗添元&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
学&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;号&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2023212455&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2023212823

指导教师&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;吴晓非&nbsp;&nbsp;&nbsp;&nbsp;
</font></font>
<font face="Times New Roman"><font size="5">2025 年 12 月</font></font>
<br><br>

</center>
</b>


## 1、系统概述
### 1.1、系统简介
&emsp;&emsp;本系统旨在开发一款智能、情感化的虚拟桌面宠物（桌宠）。它超越了传统静态或简单动画的桌宠，通过集成**大型语言模型（LLM）** 和**文本转语音（TTS）** 模型，成为一个具备"听觉"、"思考"和"语音"能力的交互式伴侣。系统的核心是赋予桌宠个性与情感：LLM负责理解用户输入，并生成符合预设角色性格、带有情感标签的对话文本；TTS模型则将文本转化为具有对应角色音色和情感的语音。应用场景包括桌面陪伴、轻度情感交流、角色扮演互动以及作为探索多模态AI应用的技术原型。

### 1.2、术语表
|序号|术语或缩略语|说明性定义|
|-|-|-|
|1|LLM|Large Language Model，大语言模型。本项目使用Qwen2-7B作为基座，通过微调使其掌握特定角色的知识和对话风格。|
|2|TTS|Text-to-Speech，文本转语音。本项目使用GPT-SoVITS模型，将LLM生成的文本合成为具有特定角色音色的自然语音。|
|3|LoRA|Low-Rank Adaptation，低秩适应。一种参数高效的微调技术，通过向预训练模型注入可训练的低秩矩阵来学习新任务，大幅减少训练参数量和计算成本。|
|4|QLoRA|结合量化与 LoRA 的低显存微调技术|
|5|FastAPI|Python Web 框架，用于模型 API 服务|
|6|GPT-SoVITS|开源的语音克隆与合成模型，用于角色音色生成|
|7|Vue 3|一款用于构建用户界面的渐进式JavaScript框架。其响应式系统和组件化架构非常适合构建动态、交互式的前端应用。|
|8|情感标签|形如`<E:smile>`、`<E:serious>`的标签，用于标注角色台词的情感状态。通过分析游戏立绘图片表情，将游戏脚本中对角色立绘切换的指令替换为对应的情感标签，使得可将游戏脚本转化为带有情感标签标注的文本，其作为LLM训练数据，可指导模型生成带情感的回复。|
|9|角色扮演AI|指能够模拟特定虚构角色性格、背景和说话方式的AI系统。本系统的核心目标之一。|

### 1.3、系统运行环境

- **硬件平台：**
    - **训练阶段：** 配备NVIDIA RTX 3090及配备RTX 4090显卡（24GB显存）的服务器，分别用于GPT-SoVITS训练，与Qwen3-8B（4 bit QLoRA）模型微调。推荐使用云平台如AutoDL、优云智算。
    - **部署/推理阶段：** 对于单组LLM&TTS模型，可运行于配备NVIDIA GPU（显存≥8GB）的服务器或个人电脑（强烈建议使用Linux（WSL2）环境）；对于多组模型强烈建议部署在服务器。不推荐在任何情况下使用CPU模式。
    - **内存：** 推荐≥24GB。
- **软件平台：**
  - **操作系统：** Ubuntu 24.04 LTS 或更高版本（推荐用于服务器），Windows 10/11（前端开发或轻量级部署）。
  - **编程语言与平台：** Python 3.10。18， Node.js 16+， CUDA 11.8（取决于平台显卡情况）。
  - **网络协议：** HTTP/HTTPS用于前后端通信，支持WebSocket（预留，用于实时音频流传输）。
  - **数据库系统：** 本项目主要使用文件系统进行数据存储，暂未引入关系型数据库。数据以JSON、JSONL、模型权重文件等形式存储。

### 1.4、开发环境
- **开发语言：**
    - **Python：** 作为后端开发与模型训练的主要编程语言，负责大语言模型加载、推理服务封装以及 API 实现。
    - **JavaScript / TypeScript：** 用于前端开发，配合 Vue 3 框架实现界面交互逻辑。
    - **HTML / CSS：** 用于前端页面结构与样式设计。
- **工具软件：**

|工具类别|工具名称|版本 / 用途说明|
|-|-|-|
|开发 IDE|Visual Studio Code <br> Cursor|主代码编辑器，支持 Python、JavaScript 等多语言开发；配合 Remote-SSH 插件，可直接连接远程服务器进行模型训练与后端开发。|
|版本控制|Git|分布式版本控制工具，用于源代码管理与协作开发，代码仓库托管于 GitHub 或 Gitee 平台。|
|Python 环境管理|Miniconda / Anaconda|用于创建和管理独立的 Python 虚拟环境，隔离不同项目依赖，避免环境冲突。|
|微调框架|LLaMA-Factory|国内开源的大语言模型微调框架，提供可视化 Web 界面，支持 QLoRA、LoRA 等参数高效微调方法，降低微调门槛。|
|深度学习库|PyTorch|2.0 及以上版本，作为核心深度学习框架，用于模型训练、参数更新与推理计算。|
|模型库|Transformers（Hugging Face）|用于加载、微调和推理各类预训练大语言模型，提供统一的模型与分词器接口。|
|后端框架|FastAPI|基于 Python 的轻量级 Web 框架，用于将大模型推理逻辑封装为 HTTP API 接口。|
|ASGI 服务器|Uvicorn|高性能 ASGI 服务器，用于运行 FastAPI 后端服务，支持异步请求处理。|
|前端框架|Vue 3|采用组合式 API 构建前端用户界面，实现虚拟桌宠交互与对话展示功能。|
|前端构建工具|Vite|下一代前端构建工具，提供极速冷启动、热更新与高效打包能力。|
|前端包管理器|npm / pnpm|用于管理前端项目依赖库与构建工具版本。|
|API 测试工具|Postman|用于测试与调试后端 API 接口，验证模型服务的正确性与稳定性。|

### 1.5、关键知识点
|序号|知识点1|关系|知识点2|
|-|-|-|-|
|1|大语言模型应用|包含|指令微调、对话生成、情感理解|
|2|语音合成技术|包含|音色克隆、语音特征提取、端到端合成|
|3|参数高效微调|是LoRA/QLoRA的|实现方法|
|4|模型服务化|通过|Web API设计 实现|
|5|软件工程|体现于|前后端分离架构|
|6|数据工程|核心是|多模态数据预处理与标注|
|7|项目管理|应用于|技术选型与迭代开发|

### 1.6、知识链条
&emsp;&emsp;**大语言模型应用 (1)** → 通过**参数高效微调 (3)** 技术（具体为QLoRA）进行个性化→ 产生带情感标签的文本（属于情感理解）→ 文本作为输入传递给**语音合成技术 (2)** → 通过音色克隆生成对应语音→ 整个流程通过**模型服务化 (4)** 提供接口→ 接口被封装在**前后端分离架构 (5)** 中→ 架构的运行依赖前期的**多模态数据预处理与标注 (6)** → 所有环节由**项目管理 (7)** 进行统筹协调。

## 2、数据结构说明
### 2.1、常量
- `BASE_MODEL: str = "Qwen/Qwen3-8B"`
    - **说明：** Hugging Face上的基座模型名称。
- `ADAPTER_DIR_ASAHI: str = "./qwen3-asa-qlora"`
    - **说明：** "小倉朝日"角色QLoRA微调后生成的适配器权重文件目录。
- `ADAPTER_DIR_LUNA: str = "./qwen3-luna-qlora"`
    - **说明：** "桜小路ルナ"角色QLoRA微调后生成的适配器权重文件目录。
- `GPT_SOVITS_MODEL_DIR: str = "./{module_name}/gsv/weights"`
    - **说明：** 训练完成的GPT-SoVITS模型文件存放目录，包含GPT_weights.ckpt和SoVITS_weights.pth，`{module_name}`在应用时固定，实际为常量。
- `AUDIO_OUTPUT_DIR: str = "./out_repl"`
    - **说明：** TTS服务生成的语音文件的输出目录。
- `MAX_HISTORY_LENGTH: int = 24 `
    - **说明：** LLM对话接口在处理请求时，所考虑的历史对话上下文的最大轮数。用于维持对话连贯性同时防止上下文过长。


### 2.2、变量

- `SYSTEM_PROMPT: str = "あなたは【{character}】として話してください。台詞は日本語で、原作の表記（「...」）を守ります。"`
    - **类型：** str
    - **说明：** LLM系统提示词模板，用于在每次对话中定义AI的角色。
- `llm_model` 和 `llm_tokenizer`
    - **类型：** transformers.PreTrainedModel， transformers.PreTrainedTokenizerFast
    - **说明：** 在FastAPI应用启动时根据加载的角色，动态加载对应的基座模型和QLoRA适配器，合并后得到的推理模型及其分词器。作为全局变量供所有聊天请求使用。
- `tts_service`
    - **类型：** TTSService (自定义类)
    - **说明：** GPT-SoVITS模型的推理服务实例，包含模型加载、参考音频设置、语音合成等方法。
- `current_character`
    - **类型：** str
    - **说明：** 当前系统激活的角色名称（如"朝日"、"ルナ"），决定调用哪一套LLM和TTS模型。
- `conversation_history`
    - **类型：** Dict[str， List[Dict]] 
    - **说明：** 在内存或简易存储中维护的用户对话历史记录，格式为消息列表。用于提供给LLM接口作为上下文。

### 2.3、数据结构

1.  **原始游戏脚本行 (Raw Script Line)**
    - **来源：** 解包后scenario/目录下的.s脚本文件。
    - **示例：**
        ~~~
        ％v_asa0001
        【小倉朝日】
        「大蔵家から紹介していただいた小倉と申します」
        ^chara03， file0:朝日/， file1:ASA_， file2:S_， file3:2_， file4:0_， file5:01
        ~~~
    - **说明：**
        - `％v_xxxx:` 语音文件ID。
        - `【角色名】:` 说话角色。
        - `「台词」:` 角色对话内容。
        - `^chara...:` 角色立绘和表情的指令参数，其中`file5:01`等数字与`face/`目录下的表情图片对应，用于情感标注。
2.  **情感映射表 (Emotion Mapping Table)**
    - **来源：** 通过分析`face/角色名/`目录下的图片文件名与脚本指令关联得出。
    - **格式 (JSON/YAML/文本):**
        ~~~
        0 <E:smile>
        1 <E:serious>
        2 <E:thinking>
        ...
        15 <E:shocked>
        ~~~
    - **说明：** 将游戏内部的表情代码（如`file5:01`）映射到统一的情感标签字符串。
3.  **中间格式台词行 (Formatted Dialogue Line)**
    - **来源：** 清洗和标注后的数据。
    - **示例 (lines.txt):**
        ~~~
        【小倉朝日】
        「大蔵家から紹介していただいた小倉と申します」
        <E:serious>
        ~~~
4.  **LLM微调数据集格式 (Training Dataset - JSONL)**
    - **来源：** 最终转换得到的训练数据。
    - **格式 (dataset.jsonl):**
        ~~~
        {
          "messages": [
            {"role": "system"， "content": "あなたは【小倉朝日】として話してください。台詞は日本語で、原作の表記（「...」）を守ります。"},
            {"role": "user"， "content": "【前後の会話】\n【山吹】\n「雇用関係を結んでない以上、まだ小倉様はお客様に当たるのですが......もし本日から採用するとなれば、私はあなたの先輩になるのですよ。初対面から会釈での挨拶とは、これはまた大型新人のご登場ですね」\n【小倉朝日】\n\n（次の台詞を、最初の行に感情タグを出力し、その次の行に台詞を出力してください。）"},
            {"role": "assistant"， "content": "<E:embarrassed>\n「あっ！　し、失礼しました！」"}
          ],
          "chat_template_kwargs": {"enable_thinking": false}
        }
        ~~~
    - **说明：**
        - `system:` 定义任务和角色。
        - `user:` 模拟的用户输入，包含历史对话和当前请求的指令，历史对话最多允许有5组。
        - `assistant:` 期望的模型输出，第一行为情感标签，第二行为纯台词。
5.  **LLM API 请求/响应结构**
    - **请求体 (POST /chat):**
        ~~~
        {
          "user": "ルナ様もそう言っていました"， // 用户当前输入（可选）
          "history": [ // 历史对话（可选）
            {"role": "user"， "content": "こんにちは"},
            {"role": "桜小路ルナ"， "content": "朝日、おはよう"},
            {"role": "小倉朝日"， "content": "<E:smile>\n「おはようございます、ルナ様」"}
          ]
        }
        ~~~
    - **响应体:**
        ~~~
        {
          "emotion": "<E:surprised>"，
          "sentence": "「............」"
        }
        ~~~
6.  **TTS API 请求/响应结构**
    - **请求:**
        `GET /speak?text=遠慮なさらずおっしゃってください%20私は絶対に笑ったりはしません&text_lang=ja`
    - **响应体:**
        ~~~
        {
          "ok": true，
          "sample_rate": 32000，
          "url": "/audio/luna_1765817571155_294edb.wav"，
          "path": "/home/lty/luna-sama/out_repl/luna_1765817571155_294edb.wav"
        }
        ~~~

## 3、模块设计
### 3.1、软件结构
~~~
1. 数据准备阶段 (离线)
   └── 模块1: 训练数据获取与处理
        ├── 输入: 游戏原始资源包 (.pack)
        ├── 过程: 解包 → 多模态数据提取与关联 → 情感标注 → 格式转换
        └── 输出:
            ├── dataset.jsonl (LLM训练数据)
            ├── emotion_mapping.json (情感标签映射)
            └── *.ogg (TTS训练数据，共6763条)

2. 模型训练阶段 (离线)
   ├── 模块2: LLM模型微调 (QLoRA on Qwen3-8B)
   │    ├── 输入: dataset.jsonl, Qwen3-8B基座模型
   │    ├── 过程: 加载配置 → 注入LoRA → 监督微调 (SFT)
   │    └── 输出: 角色专属的QLoRA适配器权重 (如 `./qwen3-asa-qlora/`)
   │
   └── 模块3: TTS模型训练 (GPT-SoVITS)
        ├── 输入: 模块1产出的角色音频文件集
        ├── 过程: 音频预处理 → SoVITS微调 → GPT微调
        └── 输出: GPT-SoVITS模型权重文件 (*.pth, *.ckpt)

3. 模型服务化阶段 (在线)
   ├── 模块4: LLM模型API服务
   │    ├── 输入: Qwen3-8B基座模型 + 模块2输出的适配器权重
   │    ├── 核心: FastAPI应用，提供 `/chat` 接口
   │    └── 输出: 带情感标签的文本回复 (JSON)
   │
   └── 模块6: TTS模型API服务
        ├── 输入: 模块3输出的GPT-SoVITS模型权重
        ├── 核心: FastAPI应用，提供 `/speak` 接口
        └── 输出: 合成语音的音频文件及URL (JSON)

4. 应用交互层 (在线)
   └── 模块5: 前端交互界面 (Vue 3)
        ├── 输入: 用户文本/指令
        ├── 过程: 
        │   1. 调用模块4 (LLM API) 获取文本与情感回复
        │   2. 解析情感标签，更新界面角色表情
        │   3. 调用模块6 (TTS API) 获取对应语音
        │   4. 播放语音并管理对话历史
        └── 输出: 图形化交互界面、语音播放、情感化角色展示

数据流与接口:
模块1 → (dataset.jsonl) → 模块2
模块1 → (音频文件) → 模块3
模块2 → (适配器权重) → 模块4
模块3 → (模型权重) → 模块6
模块5 ⇄ (HTTP API) ⇄ 模块4 与 模块6
~~~
### 3.2、功能设计说明
&emsp;&emsp;本系统采用经典的**前后端分离架构**和**模型即服务**设计理念。
1.  **前后端分离：** 前端Vue应用独立部署，只负责渲染界面、捕获用户交互、存储用户设定和播放媒体。后端FastAPI应用专注于高负载的模型推理任务。两者通过定义良好的RESTful API通信，实现了关注点分离，便于独立开发、测试、部署和扩展。
2.  **模型即服务：** 将训练好的LLM和TTS模型封装成标准的Web服务。任何具备网络通信能力的客户端（Web、移动App、其他服务）都可以通过调用API来使用模型的智能能力，极大地提高了模型的可用性和集成便利性。
3.  **流水线式多模态交互：** 用户输入文本 → 前端发送至LLM API → LLM生成带情感标签的回复文本 → 前端解析标签并更新表情 → 前端将纯文本部分发送至TTS API → 返回语音文件URL → 前端播放语音。形成一个完整的"文本理解-情感表达-语音输出"交互闭环。

### 3.3、模块1：训练数据获取
#### 3.3.1、设计图
~~~
1. 输入层
   └── 游戏原始资源包 (.xp3文件)
        ↓ [GARbro命令行/脚本化调用]
2. 解包层
   └── 解包后的目录树 (含scenario/， voice/， face/等)
        ↓ [自定义脚本遍历解析]
3. 提取与关联层
   ├── 文本流: 解析.s文件，提取【角色】和「台词」
   ├── 音频流: 根据%v_xxxx定位voice/下的.wav/.ogg文件
   └── 表情流: 解析^chara指令，映射到face/下的具体图片
        ↓ [基于规则的情感标签映射]
4. 标注层
   └── 标注后的台词列表 (角色， 台词， 情感标签)
        ↓ [对话上下文构建与格式转换]
5. 输出层
   ├── dataset.jsonl (LLM训练数据)
   └── emotion_mapping.json (情感标签映射文件)
~~~

#### 3.3.2、功能描述
&emsp;&emsp;本模块是项目的数据基石，负责将原始、非结构化的游戏资源转化为可供机器学习模型训练的高质量、结构化数据。核心任务包括：**资源解包、多模态数据（文本、音频、图像）的提取与对齐、基于规则的情感标注**，以及最终转换为标准训练数据格式。
1.  **资源解包：** 使用专业工具（GARbro）解压缩游戏资源包，获取原始的游戏脚本、音频、图像等资源。
2.  **多模态数据关联：** 从解包后的文件中，提取角色台词文本，并关联对应的语音文件和表情图片，形成"文本-语音-表情"的三元组数据。
3.  **情感标注：** 通过分析游戏脚本中的表情指令或表情图片文件命名，为每一句台词打上统一的情感标签（如`<E:smile>`， `<E:angry>`）。
4.  **格式转换：** 将标注后的数据转换为大语言模型（LLM）微调所需的标准对话格式（JSONL）。
5.  **数据清洗：** 去除无效数据、重复数据，处理编码问题，确保数据质量。

#### 3.3.3、输入数据
&emsp;&emsp;详细描述用户输入的数据(包括任何输入设备)以及这些数据的有效性检验规则。
|输入数据|来源|有效性校验|
|-|-|-|
|游戏资源包|本地文件|校验文件完整性|
|剧本文本|scenario 文件|UTF-8 编码校验|
|语音文件|voice 文件夹|WAV 格式校验|
|表情图片|face 文件夹|文件存在性校验|

- **游戏资源文件：** 视觉小说类游戏的打包文件（.pack）。
- **外部工具：** GARbro，用于解包游戏资源文件。

#### 3.3.4、输出数据
&emsp;&emsp;详细描述模块1所产生的数据以及这些数据的表现形式。
|输出数据|形式|说明|
|-|-|-|
|训练数据集|JSONL|用于 LLM 微调|
|情感标注文本|带标签字符串|用于 TTS 情感一致性|

- **主要输出：**
    1.  `combined.txt`（中间文件）：合并所有剧本文件后的原始文本，用于初始分析。
    2.  `emotion_mapping.json`：记录从游戏内部表情代码（如`file5:01`）到本项目定义的情感标签（如`<E:smile>`）的映射关系。
    3.  `lines.txt`（中间文件）：清洗后的台词与情感标签对照表，用于调试和中间检查。
    4.  `dataset.jsonl`：标准JSON Lines格式文件，每行包含一个完整的"system-user-assistant"对话样本，用于QLoRA微调Qwen3-8B模型。这是模块的核心产物。
- **数据表现形式：** 所有输出均为文本文件，采用UTF-8编码，确保跨平台兼容性。

#### 3.3.5、数据设计
- **核心数据结构：**
    - `DialogueTurn:` 表示一句完整的角色台词。
        ~~~
        class DialogueTurn:
            character: str      # 角色名，如"小倉朝日"
            line: str          # 台词文本
            emotion_code: str  # 原始表情代码，如"01"
            emotion_tag: str   # 映射后的情感标签，如"<E:serious>"
            audio_file: str    # 关联的音频文件路径，如"voice/asa0001.ogg"
            source_file: str   # 来源剧本文件
            line_num: int      # 在原文件中的行号
        ~~~
    - `Conversation:` 由多个`DialogueTurn`组成的对话片段，用于构建训练样本。
- **数据处理逻辑：**
    - **文本提取：** 解析`scenario`目录下的脚本文件（通常是明文或简单加密的文本），提取角色名和台词。
    - **音频关联：** 根据游戏脚本中的音频ID，在`voice`目录下找到对应的`.ogg`或`.wav`文件，作为TTS训练的素材。
    - **情感标注：** 通过分析脚本中调用立绘变化的指令（例如`file5:01`），将当前台词与一个表情关联。我们预定义一个映射：`file5:00 -> <E:smile>`， `file5:01 -> <E:thinking>`等。该映射规则写入`emotion_mapping.json`。
    - **数据集构建：** 模拟多轮对话。将连续的、属于同一角色的台词，结合场景描述（system），构建成用户输入-助理输出的对。助理输出必须包含情感标签。
        - **JSONL 结构示例：**
            ~~~
            {"messages": 
                [
                    {
                    "role": "system",
                    "content": "あなたは【小倉朝日】として話してください。台詞は日本語で、原作の表記（「…」）を守ります。"
                    }, 
                    {
                    "role": "user",
                    "content": "【前後の会話】\n
                                【桜小路ルナ】\n
                                「パリにはないけどな、湊ちゃんとの恋愛。おっといけない、早速ネタバレしてしまった。この先にはこんな危険がいっぱいだ」\n
                                【桜小路ルナ】\n
                                「さ、まだ大切なことを終えていなければ、一度ここから出てパリへ戻るがいい。もっともこの世界が終わる前にパリへ戻ると、今の私たちのやり取りをもう一度最初から見ることになるが」\n
                                【桜小路ルナ】\n
                                「ああ、パラレル空間と言えど、みんなそれぞれ忙しいからな。誰に会いたいんだ？」\n
                                【桜小路ルナ】\n
                                「ユルシュールか。奴は三日前に死んだ」\n
                                【小倉朝日】\n\n
                                （次の台詞を、最初の行に感情タグを出力し、その次の行に台詞を出力してください。）"
                    },
                    {
                    "role": "assistant",
                    "content": "<E:smile>\n
                                「ええええ！　一大事じゃないですか！」"
                    }
                ],
            "chat_template_kwargs": {"enable_thinking": false}
            }
            ~~~
            （为了可读性进行了手动换行）
- **存储设计：**
    - **原始数据：** 解包后的游戏资源按原目录结构存放。
    - **中间数据：** `combined.txt`， `lines.txt`等存放于`./Test&&Train/`目录。
    - **最终数据：** `dataset.jsonl`和`emotion_mapping.json`存放于`./backend/Asahi/asahi_llm/TrainData/`目录，作为训练模块的输入。
    - **音频数据：** 从`voice/`目录提取的`.wav`文件，存放于`./backend/Asahi/gsv/extracted_ogg/`目录，用于TTS模型训练。

#### 3.3.6、算法和流程

1.  **遍历脚本文件；**
2.  **提取台词文本；**
3.  **匹配对应音频与表情；**
4.  **根据表情或指令规则生成情感标签；**
5.  **清洗文本并输出 JSONL。**

#### 3.3.7、函数说明
- **`garber_extract(path: str) -> str`**
    - **文件：** `preprocess/extractor.py`
    - **功能：** 调用GARbro命令行工具，解包游戏资源文件。
    - **参数：** `path` - 游戏资源文件路径。
    - **返回值：** 解包后目录的路径。
    - **算法：** 使用`subprocess`模块执行外部命令。
- **`parse_script_file(file_path: str， encoding='shift_jis') -> List[DialogueTurn]`**
    - **文件：** `preprocess/script_parser.py`
    - **功能：** 解析剧本文件（.s），提取角色台词和表情指令。
    - **参数：** `file_path` - 剧本文件路径；`encoding` - 文件编码（通常为日文Shift_JIS）。
    - **返回值：** 从该文件中提取出的所有`DialogueTurn`对象列表。
    - **算法：** 基于行解析的状态机。识别以`【`开头的角色行，以`「`和`」`包裹的台词行，以及以`^chara`开头的指令行，并将它们关联起来。
- **`format_context_to_prompt(turns: List[DialogueTurn]) -> str`**
    - **文件：** `preprocess/formatter.py`
    - **功能：** 将一组`DialogueTurn`对象格式化为LLM训练所需的上下文提示字符串。
    - **参数：** `turns` - 上下文台词列表。
    - **返回值：** 格式化后的字符串，每行格式为`【角色】\n「台词」`。
    - **算法：** 遍历列表，拼接字符串。

#### 3.3.8 全局数据结构与该模块的关系
&emsp;&emsp;说明该模块访问了哪些全局数据结构。
&emsp;&emsp;本模块是数据生产端，不直接访问其他模块的全局数据结构。
- **产出的`dataset.jsonl`和`emotion_mapping.json`** 是后续模块2（LLM训练）和模块5（前端）的只读输入。
- **产出的音频文件列表** 是模块3（TTS训练）的输入。

### 3.4、模块2：LLM模型微调模块 (QLoRA on Qwen3-8B)
#### 3.4.1、设计图
~~~
输入:
  ├── Qwen/Qwen3-8B (来自Hugging Face Hub)
  └── dataset.jsonl (来自模块1)
        ↓
配置加载:
  ├── 加载原项目LoRA配置 (adapter_config.json)
  └── 加载原项目聊天模板 (chat_template.jinja)
        ↓
模型与分词器准备:
  ├── 初始化Tokenizer并注入聊天模板
  ├── 配置4-bit量化 (BitsAndBytesConfig)
  ├── 加载量化后的基座模型
  └── 应用LoRA适配器配置 (PEFT)
        ↓
数据预处理:
  ├── 加载dataset.jsonl
  ├── 应用聊天模板，将对话消息转为单一文本字符串
  └── 进行分词 (Tokenization)
        ↓
训练循环 (SFTTrainer):
  ├── 前向传播与损失计算
  ├── 反向传播与梯度更新 (仅更新LoRA参数)
  └── 周期性保存检查点
        ↓
输出:
  └── 适配器权重目录 (包含 adapter_model.bin， config.json 等)
~~~

#### 3.4.2、功能描述
&emsp;&emsp;使用QLoRA技术，在预训练的Qwen3-8B大语言模型基础上，利用模块1产生的角色对话数据集进行监督微调（SFT）。目标是让模型学会：
1.  以特定角色（如"小倉朝日"）的口吻和知识背景进行对话。
2.  在生成回复时，能够输出符合上下文情感的情感标签（如`<E:embarrassed>`）。
3.  遵守指令，严格按照"情感标签+台词"的格式输出。

#### 3.4.3、输入数据
1.  **基座模型：** 从Hugging Face Hub下载的`Qwen/Qwen3-8B`模型。
2.  **训练数据：** 模块1产出的`dataset.jsonl`文件。
3.  **配置文件：**
    - `adapter_config.json`：定义LoRA的rank(r)， alpha， target_modules等超参数。
    - `chat_template.jinja`：定义将对话消息列表转换为模型输入文本的模板。这是确保训练/推理格式一致的关键。
4.  **训练超参数：** 学习率(lr=2e-4)、批大小(batch_size=2)、训练轮数(epochs=3）等，在训练脚本中硬编码或通过配置文件指定。

#### 3.4.4、输出数据
- **主要输出：**
    - **适配器权重目录** (如`./qwen3-asa-qlora/`): 包含训练好的LoRA权重文件(`adapter_model.safetensors`或`.bin`)、PEFT配置(`adapter_config.json`)、以及保存的分词器(`tokenizer.json`)和聊天模板(`chat_template.jinja`)。
    - **训练日志：** 记录损失曲线、GPU使用情况等，用于监控和调试。
- **注意：** QLoRA训练后得到的是适配器（Adapter）权重，而非完整的模型权重。部署时需要将适配器与原始基座模型动态结合。

#### 3.4.5、数据设计
- **训练时数据流：** 原始消息字典 -> 应用聊天模板 -> 单一文本字符串 -> 分词器Tokenization -> 模型输入ID序列。
- **损失计算：** 标准语言建模损失，模型需要预测序列中的下一个token。通过`attention_mask`确保只对"助理回复"部分计算损失。

#### 3.4.6、算法和流程
1.  **环境准备：** 安装依赖，创建环境。
2.  **加载配置：** 从原项目加载LoRA配置和聊天模板。
3.  **初始化分词器：** 加载Qwen3-8B的分词器，并将关键的`chat_template`设置进去，确保训练和推理时格式一致。
4.  **配置量化与加载模型：** 使用`BitsAndBytesConfig`配置4-bit量化，然后加载量化后的基座模型。
5.  **应用PEFT：** 使用`prepare_model_for_kbit_training`准备模型，并用`LoraConfig`创建LoRA配置，通过`get_peft_model`将LoRA适配器注入模型。
6.  **准备数据集：** 使用`load_dataset`加载`dataset.jsonl`。定义`formatting_prompts_func`函数，该函数使用`tokenizer.apply_chat_template`将每个样本的`messages`字段转换为格式化的文本。然后使用`dataset.map`方法，应用一个将格式化文本进行分词的处理函数，得到`tokenized_dataset`。
7.  **配置训练器：** 使用`SFTTrainer`，传入模型、分词器、训练参数和预处理好的`tokenized_dataset`。`SFTTrainer`内部会自动处理数据整理（collate）。
8.  **开始训练：** 调用`trainer.train()`。
9.  **保存模型：** 训练完成后，调用`trainer.save_model()`保存适配器权重和相关配置。

#### 3.4.7、函数说明
- **`load_original_config(config_dir)`**
    - **功能：** 加载已有的LoRA配置和聊天模板，确保微调与现有部署兼容。
    - **关键作用：** 复用`target_modules`等关键配置，避免因配置不同导致适配器无法加载。
- **`formatting_prompts_func(example)`** 和 **`preprocess_and_tokenize(example)`**
    - **功能：** 数据预处理的核心。前者将对话消息转为文本，后者进行分词。
    - **关键细节：** 必须使用与推理API (`api.py`) 完全相同的 `tokenizer.apply_chat_template` 逻辑，这是项目成功的经验之一。

#### 3.4.8 全局数据结构与该模块的关系
1.  **消费模块1产出的`dataset.jsonl`。**
2.  **产出适配器权重文件**，供模块4（LLM API服务） 加载使用。

### 3.5、模块3：TTS模型训练模块 (GPT-SoVITS)
#### 3.5.1、功能描述
&emsp;&emsp;使用从游戏`voice/`目录提取的特定角色的音频文件（共6763条），训练GPT-SoVITS模型，以实现对该角色音色的高质量克隆。训练后的模型能够将任意文本合成为该角色的语音。

#### 3.5.2、关键流程
1.  **数据准备：** 使用GPT-SoVITS工具进行音频切分、降噪、语音识别（ASR）转文本。
2.  **特征提取：** 提取音频的Hubert语义特征和声学特征。
3.  **两阶段训练：**
    - **SoVITS微调：** 训练声音转换器部分，学习角色音色。
    - **GPT微调：** 训练文本-语音对齐生成器部分。
4.  **输出：** 得到两个权重文件：`SoVITS_weights.pth` (例：`ASA_e8_s6648.pth`) 和 `GPT_weights.ckpt` (例：`ASA-e15.ckpt`)。

#### 3.5.3、与系统的关系
- 训练流程相对独立，使用官方可视化工具进行。
- 产出最终的`*.pth`和`*.ckpt`文件，作为模块6（TTS API服务） 的模型输入。

### 3.6、模块4：LLM模型API服务模块
#### 3.6.1、设计图
~~~
FastAPI 应用 (api.py)
├── 启动事件 (on startup)
│   └── 根据命令行参数或配置，加载指定角色的:
│       ├── Qwen3-8B 基座模型 (4-bit量化)
│       ├── QLoRA 适配器权重
│       ├── Tokenizer (已注入chat_template)
│       └── 初始化全局变量 `model`， `tokenizer`
├── 路由端点 (Routes)
│   ├── GET /info -> 返回服务元数据 (角色、端口、模型信息)
│   └── POST /chat -> 核心对话接口
│       ├── 1. 接收请求体 (用户输入， 历史)
│       ├── 2. 格式化输入 (调用 format_conversation_history)
│       ├── 3. 构建消息列表 [system， user]
│       ├── 4. 应用聊天模板，生成模型输入tokens
│       ├── 5. 调用 model.generate() 进行推理
│       ├── 6. 解码输出，解析情感标签和台词 (parse_model_output)
│       └── 7. 返回 JSON 响应 {emotion， sentence}
└── 运行服务器 (Uvicorn)
~~~

#### 3.6.2、功能描述
&emsp;&emsp;将训练好的角色专属LLM（基座模型+LoRA适配器）封装成一个可通过HTTP调用的Web服务。提供`/chat`接口接收用户请求，返回符合角色设定的带情感标签的文本回复。

#### 3.6.3、核心算法：format_conversation_history
&emsp;&emsp;此函数是将前端传入的灵活历史记录格式，转换为与训练数据格式严格一致的提示词的关键。
- **输入：** `history` (消息列表)， `current_input` (可选)
- **逻辑：**
    1.  如果提供了`history`，则将其最后N条（如24条）格式化为`【角色】\n「内容」`的形式，并在开头加上`【前後の会話】`。
    2.  如果提供了`current_input`，则将其作为`【ユーザー】`的台词加入。
    3.  最后，追加固定指令：`\n【{角色名}】\n\n（次の台詞を、最初の行に感情タグを出力し、その次の行に台詞を出力してください。）`
- **输出：** 一个字符串，直接作为`user`角色的`content`，与`system`提示一起构成完整的`messages`列表，送入`tokenizer.apply_chat_template`。

#### 3.6.4、接口灵活性
&emsp;&emsp;如文档所示，`/chat`接口设计灵活，支持三种调用模式：
1.  **用户输入+历史记录：** 最常见的交互模式。
2.  **仅历史记录** (`user`字段为空)：让AI根据历史对话自动续写下一个角色的台词。这是一个巧妙的设计，可用于自动推进对话。
3.  **仅用户输入** (`history`为空)：单轮对话。

#### 3.6.5、部署
&emsp;&emsp;每个角色独立运行一个服务实例，绑定不同端口（如朝日:8001， ルナ:8000）。通过命令行参数`--port`指定端口，增强灵活性。

### 3.7、模块5：前端交互模块 (Vue 3)
#### 3.7.1、功能描述
&emsp;&emsp;构建用户与虚拟桌宠交互的图形界面。核心功能包括：
1.  **角色选择与切换：** 在"朝日"和"ルナ"等角色间切换。
2.  **对话界面：** 显示对话历史气泡（区分用户和AI），提供文本输入框和发送按钮。
3.  **情感可视化：** 解析AI回复中的情感标签（如`<E:smile>`），并切换桌宠的立绘表情或播放相应微动画。
4.  **语音交互：** 自动将AI的文本回复发送至TTS服务，获取并播放语音。
5.  **对话历史管理：** 在本地（如LocalStorage）保存对话历史，以便刷新后恢复。

#### 3.7.2、与后端接口的协同流程
~~~js
async function callModelLLM(client, userInput, historyArray = []) {
  const payload = {
    user: userInput || '',
    history: historyArray || []
  }
  const res = await client.post('/chat', payload)
  // 兼容不同返回结构
  const data = res?.data || res
  return data?.sentence || data?.data?.sentence || ''
}

async function callModelTTS(client, text, text_lang, proxyBase) {
  const res = await client.get('/speak', { params: { text, text_lang } })
  const data = res?.data || res
  const rawDownload =
    data?.download_url ||
    data?.data?.download_url ||
    data?.download_info?.download_url ||
    data?.data?.download_info?.download_url ||
    data?.url ||
    data?.data?.url
  const downloadUrl = sanitizeDownloadUrl(rawDownload, proxyBase)
  const { audioUrl, ok, error } = await downloadWavWithRetry(downloadUrl)
  return {
    downloadUrl,
    audioUrl,
    ok,
    error
  }
}

export async function callLunaLLM(userInput, historyArray) {
  return callModelLLM(lunaLLMClient, userInput, historyArray)
}

export async function getLunaTTS(text, text_lang = 'ja') {
  return callModelTTS(lunaTTSClient, text, text_lang, LUNA_TTS_BASE)
}

export async function callAsahiLLM(userInput, historyArray) {
  return callModelLLM(asahiLLMClient, userInput, historyArray)
}

export async function getAsahiTTS(text, text_lang = 'ja') {
  return callModelTTS(asahiTTSClient, text, text_lang, ASAHI_TTS_BASE)
}

~~~

#### 3.7.3、关键处理点
1.  **历史记录格式：** 前端存储的`history`需要与LLM API要求的格式一致：`{role: ‘user’/’assistant’/角色名， content: ‘文本’}`。AI的`content`在存入历史时，应只保存纯台词（不含情感标签）。
2.  **情感标签解析：** 从LLM返回的`emotion`字段获取标签，用于控制前端表情。
3.  **音频下载的可靠性：** 实现重试机制（如重试3次），并处理下载失败情况，否则降级为仅显示文本。

### 3.8、模块6：TTS模型API服务模块
#### 3.8.1、功能描述
&emsp;&emsp;将训练好的GPT-SoVITS模型封装为Web服务。提供`/speak`接口，接收文本和参数，返回合成语音的音频文件URL。

#### 3.8.2、设计特点 (基于改进后的gsv/api.py)
1.  **云端部署友好：** 服务启动时需通过`--ref_wav`， `--ref_text`等参数指定参考音频，合成结果以文件形式输出到`--out_dir`目录，并通过`StaticFiles`挂载为静态资源提供下载。
2.  **线程安全：** 使用`threading.Lock`确保语音合成过程线程安全。
3.  **接口简洁：** `GET /speak`接口只需传入`text`和`text_lang`等必要参数，参考音频在服务初始化时固定。
4.  **动态参考切换 (可选)：** 保留了`/set_ref`接口，支持运行时更新说话人参考音频，增强了灵活性。

#### 3.8.3、与前端交互
- **前端通过`GET`请求调用**，参数放在URL查询字符串中。
- **返回的`url`字段是服务器相对路径**，前端需要根据实际部署的服务器IP进行拼接，才能正确下载。

## 4、 接口设计
### 4.1、 用户接口

- **图形界面 (GUI):**
    - **桌面窗口：** 包含角色头像（带表情变化）、对话历史展示区域（气泡式）、文本输入框、发送按钮、角色切换下拉菜单。
    - **交互：** 支持文本输入、点击发送、点击切换角色。
- **功能接口:**
    - **文本对话：** 输入日文文本，获取带情感的文本回复和对应语音。
    - **角色切换：** 在"桜小路ルナ"和"小倉朝日"等角色间切换，系统自动连接对应的后端API。

### 4.2、 外部接口
1.  **模型源：** Hugging Face Hub，用于下载Qwen3-8B基座模型。
2.  **云平台API：** AutoDL的API，用于在训练和部署阶段管理云服务器实例、传输数据。

### 4.3、 内部接口
#### 4.3.1、 接口说明
1.  **LLM服务接口** (`http://<ip>:8000` 或 `8001`):
    - `GET /info`： 获取服务基本信息（角色、模型等）。
    - `POST /chat`： 核心对话接口，接收JSON请求，返回JSON格式的回复。
2.  **TTS服务接口** (`http://<ip>:9880` 或 `9881`):
    - `GET /speak`： 语音合成接口，接收文本参数，返回包含音频文件URL的JSON。
    - `GET /set_ref` (可选)： 动态更新参考音频。
3.  **前端与后端：** 前端通过HTTP调用上述LLM和TTS接口。

#### 4.3.2、 调用方式（同上）
~~~js
async function callModelLLM(client, userInput, historyArray = []) {
  const payload = {
    user: userInput || '',
    history: historyArray || []
  }
  const res = await client.post('/chat', payload)
  // 兼容不同返回结构
  const data = res?.data || res
  return data?.sentence || data?.data?.sentence || ''
}

async function callModelTTS(client, text, text_lang, proxyBase) {
  const res = await client.get('/speak', { params: { text, text_lang } })
  const data = res?.data || res
  const rawDownload =
    data?.download_url ||
    data?.data?.download_url ||
    data?.download_info?.download_url ||
    data?.data?.download_info?.download_url ||
    data?.url ||
    data?.data?.url
  const downloadUrl = sanitizeDownloadUrl(rawDownload, proxyBase)
  const { audioUrl, ok, error } = await downloadWavWithRetry(downloadUrl)
  return {
    downloadUrl,
    audioUrl,
    ok,
    error
  }
}

export async function callLunaLLM(userInput, historyArray) {
  return callModelLLM(lunaLLMClient, userInput, historyArray)
}

export async function getLunaTTS(text, text_lang = 'ja') {
  return callModelTTS(lunaTTSClient, text, text_lang, LUNA_TTS_BASE)
}

export async function callAsahiLLM(userInput, historyArray) {
  return callModelLLM(asahiLLMClient, userInput, historyArray)
}

export async function getAsahiTTS(text, text_lang = 'ja') {
  return callModelTTS(asahiTTSClient, text, text_lang, ASAHI_TTS_BASE)
}

~~~

## 5、数据库设计
&emsp;&emsp;使用本地存储设计，未采用数据库。

## 6、系统出错处理
### 6.1、 出错信息
|错误场景|可能原因|系统响应 / 日志记录|用户可见提示|处理建议|
|-|-|-|-|-|
|LLM API `/chat` 请求失败|网络异常、后端服务未启动或服务进程崩溃|前端捕获网络异常并在控制台输出错误信息；后端日志记录无效请求或内部错误堆栈|无法连接到 AI 服务，请检查网络或稍后重试|前端实现有限次数的自动重试机制（如 2 次）；检查后端服务进程运行状态|
|TTS API `/speak` 请求或音频下载失败|网络异常、TTS 服务故障、语音合成失败|前端捕获异常；TTS 服务返回非 200 状态码或错误 JSON 信息|语音生成失败，将仅显示文字回复|前端对音频下载实现带延迟的重试机制（如 3 次）；若失败则忽略语音，不阻塞主流程|
|LLM 生成内容格式不符合预期|模型未充分对齐、提示词被绕过或异常输出|后端 `parse_model_output` 函数解析失败并返回默认值；日志记录异常原始输出|情感标签缺失或台词显示异常|增强后端输出解析的鲁棒性；检查训练数据格式与提示词模板的一致性|
|显存不足（OOM）|同时加载多个模型、输入文本过长、显存资源不足|后端服务启动失败或推理过程中崩溃；系统日志记录 CUDA OOM 错误|服务暂时不可用|部署前确认服务器显存资源充足；在前端或网关层限制输入文本长度；采用更高效的量化方案|
|输入文本不符合语言预期|用户输入语言不符合模型训练语料|模型生成内容质量下降，可能出现语义偏差|回复内容不自然或质量下降|前端可增加输入语言提示或简单校验，但不强制限制用户输入|

### 6.2、 补救措施
1.  **服务进程守护：** 在云服务器上使用`systemd`或`supervisord`管理LLM/TTS服务进程，配置自动重启。
2.  **健康检查与熔断：** 前端或网关可定时调用`/info`或`/health`端点检查服务健康。连续失败后，暂时熔断对该服务的请求，避免雪崩。
3.  **资源监控与告警：** 监控服务器GPU显存、CPU和内存使用率。设置阈值告警，以便人工及时干预。
4.  **日志与排查：** 所有服务记录详细日志（请求、响应、错误），便于快速定位问题。
5.  **数据备份：** 定期备份训练好的模型权重文件，防止误删或损坏。

## 7、其他设计
### 7.1、系统安全设计
- **API安全：**
    - **CORS：** 在生产环境中，应在前端部署的域名或本地`localhost`严格配置CORS，而非允许所有来源(`*`)。
    - **速率限制：** 对`/chat`和`/speak`接口实施IP级或用户级的速率限制，防止滥用。
    - **输入过滤：** 对用户输入进行基本的清理，防止潜在的注入攻击（尽管对LLM直接影响小）。
- **部署安全：**
    - **反向代理：** 由于使用了百度的大模型翻译接口，故使用Nginx作为反向代理，隐藏后端服务的直接端口，并方便配置SSL/TLS(HTTPS)。
    - **非特权运行：** 后端服务进程不应以`root`用户身份运行。
    - **敏感信息：** 服务器IP、端口等配置信息不应硬编码在前端代码中，可通过构建时注入或配置文件读取。

### 7.2、系统性能设计
- **模型推理优化：**
    - **4-bit量化：** 部署时使用QLoRA的4-bit量化加载LLM，极大降低显存占用。
    - **缓存：** 对于完全相同的TTS文本请求，可在服务端内存中缓存一小段时间内的结果，避免重复合成。
- **前端性能：**
    - **虚拟列表：** 如果对话历史很长，使用虚拟滚动技术渲染对话气泡，避免DOM节点过多。
    - **资源懒加载：** 角色表情图片等资源按需加载。
- **可扩展性：**
    - **微服务架构：** 当前LLM和TTS已按角色分离为独立服务，为水平扩展打下基础。未来负载增加时，可以为同一角色启动多个服务实例，并通过负载均衡器分配流量。
    - **无状态服务：** API服务设计为无状态的，方便扩展和重启。会话状态由客户端或外部存储管理。