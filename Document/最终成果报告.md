<center>

![alt text](image.png)
# <font face="黑体">《程序设计实验》最终成果报告
![alt text](image-1.png)
<br>

### 题目：<u>"AI灵宠"：面向陪伴交互的多模型虚拟桌宠系统开发</u></font>

<font face="等线"><font size="5"><b> 
班&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;级&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2024219105&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2024219104
姓&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;名&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乐长昕&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;罗添元&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
学&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;号&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2023212455&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2023212823

指导教师&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;吴晓非&nbsp;&nbsp;&nbsp;&nbsp;
</font></font>
<font face="Times New Roman"><font size="5">2025 年 12 月</font></font>
<br><br>

</center>
</b>

## 目录

1. [项目概述](#1项目概述)
2. [需求分析](#2需求分析)
3. [项目管理](#3项目管理)
4. [系统设计](#4系统设计)
5. [核心模块实现](#5核心模块实现)
6. [开发环境与部署流程](#6开发环境与部署流程)
7. [测试与验证](#7测试与验证)
8. [项目成果与展望](#8项目成果与展望)
9. [团队分工与贡献率](#9团队分工与贡献率)

---

## 1、项目概述

### 1.1、背景与动机

&emsp;&emsp;当前市场上的虚拟桌宠应用主要分为两类：一类具备基础的交互对话功能，但功能单一、缺乏个性化定制能力；另一类侧重于实用工具功能，但缺少情感化、拟人化的交互体验。本项目旨在突破这一局限，开发一款集智能对话、情感表达、语音交互于一体的多模型虚拟桌宠系统，为用户提供真正有温度、有个性的数字伴侣。

&emsp;&emsp;本系统以日本视觉小说游戏《月に寄りそう乙女の作法》（近月少女的礼仪）为数据源，通过深度学习技术训练特定角色的大语言模型（LLM）和文本转语音模型（TTS），实现了高度拟真的角色扮演AI。不同于传统的通用聊天机器人，本系统中的虚拟角色具有鲜明的个性、一致的说话风格，并能根据上下文生成带有情感标签的回复，配合对应情感的语音输出，为用户打造沉浸式的交互体验。

&emsp;&emsp;项目已开源至：[firefly163/AI-Project](https://github.com/firefly163/AI-Project)

### 1.2、项目目标

&emsp;&emsp;本项目的核心目标是开发一个技术先进、功能完整、用户体验优秀的虚拟桌宠系统。具体目标包括：

1. **技术创新**：
   - 实现从游戏原始资源到训练数据的完整数据工程pipeline
   - 基于QLoRA技术对Qwen3-8B大语言模型进行参数高效微调
   - 训练GPT-SoVITS模型实现高质量的角色音色克隆
   - 构建前后端分离的分布式服务架构

2. **功能完整**：
   - 支持多角色（桜小路ルナ、小倉朝日）的独立对话
   - 实现文本生成、情感识别、语音合成的完整交互流程
   - 提供友好的Web界面，支持对话历史管理、多语言翻译等扩展功能

3. **性能优化**：
   - 通过4-bit量化技术降低模型显存占用
   - 支持云端和本地两种部署模式
   - 优化推理速度，确保流畅的用户体验

### 1.3、系统特色

|特色功能|技术实现|用户价值|
|-|-|-|
|**情感化对话**|LLM输出带情感标签的文本，驱动前端表情切换|角色回复不仅有语义，更有情感温度|
|**音色克隆**|基于6763条游戏原声训练GPT-SoVITS|语音输出高度还原原作角色音色|
|**个性化角色**|每个角色独立微调的LLM和TTS模型|不同角色具有独特的说话风格和性格|
|**云端部署**|后端服务部署在云服务器，前端可本地或远程访问|降低用户硬件门槛，随时随地访问|
|**对话连贯性**|支持最多24轮对话历史上下文|角色能够记住对话内容，实现连贯交流|
|**多语言支持**|集成百度翻译API|中日英多语言用户均可无障碍交流|

### 1.4、技术栈概览

- **后端技术**：Python 3.10, FastAPI, PyTorch 2.8, Transformers 4.57, PEFT 0.18, GPT-SoVITS
- **前端技术**：Vue 3, Vite, Pinia, Axios, Element Plus
- **模型技术**：Qwen3-8B (QLoRA微调), GPT-SoVITS (音色克隆)
- **部署技术**：Uvicorn, Conda环境管理, Shell脚本自动化, 云服务器(AutoDL)

### 1.5、项目结构概览

```
AI-Project/
├── Document/                        # 项目文档
│   ├── 最终成果报告.md              # 本文档
│   ├── 详细设计.md                  # 技术详细设计
│   ├── 概要设计.md                  # 系统概要设计
│   └── 需求设计.md                  # 需求分析文档
│
├── backend/                         # 后端服务
│   ├── Asahi/                       # "小倉朝日"角色服务
│   │   ├── asahi_llm/               # LLM服务
│   │   │   ├── api.py               # FastAPI服务端点
│   │   │   └── qwen3-asa-qlora/     # QLoRA适配器权重
│   │   ├── gsv/                     # GPT-SoVITS服务
│   │   │   ├── api.py               # TTS服务端点
│   │   │   ├── weights/             # 训练后的模型权重
│   │   │   └── pretrained_models/   # 预训练模型（大文件）
│   │   ├── run_api_llm.sh           # LLM启动脚本
│   │   ├── run_api_sovits.sh        # TTS启动脚本
│   │   └── requirements.txt         # Python依赖
│   │
│   ├── luna-sama/                   # "桜小路ルナ"角色服务（结构同上）
│   │   ├── luna_llm/
│   │   ├── gsv/
│   │   ├── run_api_llm.sh
│   │   └── run_api_sovits.sh
│   │
│   ├── OverSizedFiles/              # 大文件集中存放
│   │   ├── adapter_model.safetensors  # QLoRA适配器（~200MB）
│   │   ├── ASA_sovits_e8_s6648.pth   # SoVITS权重（~60MB）
│   │   ├── ASA-gpt-e15.ckpt          # GPT权重（~800MB）
│   │   ├── pytorch_model.bin         # 预训练权重（~1GB）
│   │   └── ...                       # 其他大文件
│   │
│   └── where-to-put.txt  # 大文件路径映射表
│
├── frontend/                        # 前端应用
│   ├── src/
│   │   ├── views/content/           # 页面组件
│   │   │   └── chat/ChatPage.vue    # 聊天界面核心组件
│   │   ├── api/
│   │   │   └── models.js            # 后端API封装
│   │   ├── stores/                  # Pinia状态管理
│   │   └── utils/                   # 工具函数（翻译、i18n等）
│   ├── package.json
│   └── vite.config.js               # Vite配置（含API代理）
│
├── Test&&Train/                     # 数据处理脚本
│   ├── textProcessor1-toCombined.py      # 合并所有剧本
│   ├── textProcessor2-tagCounter.py      # 统计情感代码
│   ├── textProcessor3-toLines.py         # 提取台词+情感
│   ├── textProcessor4-fix3.py            # 数据清洗
│   ├── textProcessor5-todataset.py       # 生成JSONL训练数据
│   ├── dataset.jsonl                     # 最终训练数据
│   ├── EmotionMap.xlsx                   # 情感标签映射表
│   └── train4090.py                      # QLoRA训练脚本
│
├── serverlog.txt                    # 云服务器运行日志
├── README.md                        # 项目说明
└── LICENSE                          # MIT许可证
```

**关键文件说明**：

1. **大文件管理**：
   - 由于Git对单文件大小有限制（GitHub<100MB），我们将所有大文件集中存放在`backend/OverSizedFiles/`目录
   - `where-to-put.txt`记录了每个大文件应复制到的最终位置
   - 实际部署时需从百度网盘下载这些文件并按路径映射表放置

2. **API版本差异**：
   - 本地版本API：音频保存在本地路径，返回本地文件路径
   - 云端版本API：音频保存在服务器，返回可下载的HTTP URL
   - 两者代码基本相同，仅URL构造逻辑有差异

3. **文档关系**：
   - `详细设计.md`记录了实际的技术实现
   - `最终成果报告.md`（本文档）基于详细设计扩展而成，补充了项目管理、测试和成果总结
   - 概要设计和需求设计提供了格式参考

---

## 2、需求分析

### 2.1、功能需求

#### 2.1.1、核心功能需求

&emsp;&emsp;根据项目目标，系统需要实现以下核心功能：

**1. 智能对话功能**
- **输入**：用户文本输入（中/日文）
- **处理**：基于角色设定、对话历史和大语言模型进行上下文理解、意图识别、情感推理和回复生成
- **输出**：带情感标签的文本回复（格式：`<E:emotion>\n「台词」`）
- **要求**：回复内容符合角色性格设定，情感标签与台词内容一致，对话风格自然流畅

**2. 语音合成功能**
- **输入**：LLM生成的纯文本台词
- **处理**：调用GPT-SoVITS模型，基于预设的参考音频进行语音合成
- **输出**：WAV格式音频文件及其下载URL
- **要求**：音色高度还原原作角色，发音清晰自然，情感表现力强

**3. 多角色支持**
- 系统同时支持至少两个独立角色（桜小路ルナ、小倉朝日）
- 每个角色拥有独立的LLM和TTS服务实例，绑定不同端口
- 前端可灵活切换对话角色

**4. 对话历史管理**
- 系统能够维护和传递对话历史上下文（最多24轮）
- 支持历史记录的本地存储和恢复
- LLM接口能够灵活处理不同格式的历史记录输入

#### 2.1.2、扩展功能需求

**1. 多语言翻译**
- 集成第三方翻译API（百度翻译）
- 支持中文、日文、英文之间的双向翻译
- 提供悬停气泡式翻译UI

**2. 自动转发**
- 检测AI回复中是否提及其他角色
- 若检测到，自动将消息转发给对应角色，触发连续对话
- 实现多角色间的自然互动

**3. 用户界面定制**
- 支持深色/浅色/系统主题切换
- 支持中文/英文/日文界面语言切换
- 允许用户自定义角色头像、名称等

### 2.2、非功能需求

#### 2.2.1、性能需求

||需求指标|实现方式|
|-|-|-|
|**响应时间**|LLM推理延迟<3秒|使用4-bit量化、优化generate参数|
|**吞吐量**|单个服务实例支持并发请求|FastAPI异步处理、线程锁保护关键资源|
|**显存占用**|单个QLoRA模型显存<8GB|QLoRA + 4-bit量化（NF4）|
|**音频质量**|采样率32kHz，无明显噪音和失真|GPT-SoVITS高质量训练，合理设置推理参数|

#### 2.2.2、可靠性需求

- **服务稳定性**：后端服务需具备异常处理机制，单次推理失败不应导致服务崩溃
- **数据一致性**：对话历史应实时保存，页面刷新后能够恢复
- **容错能力**：TTS音频下载失败时，系统应降级为纯文本展示，不阻塞主流程
- **网络鲁棒性**：前端实现音频下载重试机制（默认3次），增强对网络波动的抵抗能力

#### 2.2.3、可用性需求

- **用户友好界面**：提供直观的图形界面，气泡式对话展示，支持流式打字机效果
- **操作简便**：无需复杂配置，用户仅需访问Web页面即可开始对话
- **文档完善**：提供详尽的部署文档、API接口说明和使用教程
- **错误提示**：网络异常、服务不可用等情况应有清晰的错误提示

#### 2.2.4、扩展性需求

- **模块化设计**：前后端分离，LLM与TTS服务解耦，便于独立升级
- **新角色接入**：预留标准化接口，方便后续添加新角色
- **模型热替换**：支持在不停机的情况下更新模型参数（通过重启特定服务实现）
- **API标准化**：采用RESTful API设计，便于第三方集成

### 2.3、约束条件

#### 2.3.1、技术约束

- **硬件要求**：训练阶段需要NVIDIA RTX 3090/4090级别GPU；推理阶段至少需要8GB显存的GPU
- **语言限制**：当前模型仅支持日文输入输出，中英文需通过翻译API处理
- **数据版权**：训练数据来源于特定游戏，仅供学习研究使用，不可用于商业用途
- **依赖环境**：需要Python 3.10、CUDA 11.8以上环境，依赖较多深度学习库

#### 2.3.2、时间约束

- **项目周期**：约16周（2025年9月-12月）
- **模型训练**：LLMd的QLoRA微调约2-3小时（4090），GPT-SoVITS训练约1-2小时（3090）
- **部署周期**：首次训练需约半天时间，部署和配置约1~2个小时

#### 2.3.3、成本约束

- **云服务器成本**：使用AutoDL或类似云平台，按小时计费
- **API调用成本**：百度翻译API需申请并可能产生费用
- **人力成本**：2人团队，工作量分配需合理规划

---

## 3、项目管理

### 3.1、开发流程

&emsp;&emsp;本项目采用**敏捷开发**模式，将整体任务划分为多个迭代周期，每个周期包含需求分析、设计、开发、测试和部署阶段。通过快速迭代和持续反馈，逐步完善系统功能。

#### 3.1.1、开发阶段划分

**第一阶段：需求分析与技术调研（第1-2周）**
- 明确项目目标和功能需求
- 调研大语言模型微调技术（QLoRA、PEFT）
- 调研语音合成技术（GPT-SoVITS）
- 确定技术栈和系统架构
- 制定项目计划和任务分工

**第二阶段：数据获取与预处理（第3-5周）**
- 使用GARbro工具解包游戏资源
- 分析游戏脚本结构，设计数据提取方案
- 编写数据预处理脚本（textProcessor系列）
- 构建情感标签映射表
- 生成最终训练数据集（dataset.jsonl）

**第三阶段：模型训练（第6-9周）**
- 配置训练环境（AutoDL云平台）
- GPT-SoVITS音频预处理与特征提取
- GPT-SoVITS模型微调训练
- 编写QLoRA训练脚本
- Qwen3-8B模型微调训练
- 模型效果测试与参数调优

**第四阶段：后端服务开发（第8-11周）**
- 设计LLM API接口（/chat、/info）
- 设计TTS API接口（/speak、/set_ref）
- 实现模型加载与推理逻辑
- 优化在线版本API（支持云端部署）
- 编写启动脚本（Shell）
- 本地和云端部署测试

**第五阶段：前端开发（第10-13周）**
- 搭建Vue 3项目框架
- 开发聊天页面UI组件
- 实现对话历史管理
- 集成LLM和TTS API
- 开发翻译和自动转发功能
- 实现主题和语言切换
- 优化用户体验和界面美化

**第六阶段：系统集成与测试（第14-15周）**
- 前后端联调
- 端到端功能测试
- 性能测试和优化
- 边界情况和异常处理测试
- 撰写用户文档

**第七阶段：文档撰写与项目总结（第16周）**
- 撰写需求分析、概要设计、详细设计文档
- 撰写最终成果报告
- 整理项目代码和资源
- 准备项目演示和答辩

### 3.2、技术选型

#### 3.2.1、大语言模型选型

&emsp;&emsp;经过对比多种开源大语言模型，我们选择了**Qwen3-8B**作为基座模型。

**选择理由**：
1. Qwen3-8B在中日英等多语言任务上表现优异，特别是日文处理能力强
2. 支持丰富的chat template定制，便于适配我们的情感标签格式
3. 与PEFT、BitsAndBytes等库兼容性好，支持高效的QLoRA微调
4. 模型较新（2024年发布），社区活跃，文档完善

#### 3.2.2、语音合成模型选型

&emsp;&emsp;我们选择了**GPT-SoVITS v2ProPlus**作为TTS模型：

**选择理由**：
1. **音色克隆能力强**：仅需少量参考音频即可复现目标音色
2. **训练友好**：提供完整的可视化训练流程，降低技术门槛
3. **效果优秀**：v2ProPlus版本在音质、自然度、情感表现力上表现出色
4. **部署灵活**：支持API服务化，易于集成到我们的系统中
5. **开源免费**：MIT许可证，适合学术研究

#### 3.2.3、前端框架选型

&emsp;&emsp;前端采用**Vue 3 + Vite**技术栈：

**选择理由**：
1. **Vue 3**：组合式API编程范式清晰，响应式系统高效，适合构建复杂交互界面
2. **Vite**：极速冷启动和热更新，开发体验优秀
3. **Pinia**：新一代Vue状态管理库，API简洁，TypeScript支持好
4. **Element Plus**：成熟的Vue 3 UI组件库，提供丰富的组件和良好的文档

### 3.3、版本控制与协作

&emsp;&emsp;项目使用**Git**进行版本控制，代码托管在GitHub平台。

#### 3.3.1、仓库管理策略

**主仓库**：[firefly163/AI-Project](https://github.com/firefly163/AI-Project)
- 存放完整的项目代码（前端+后端+文档）
- 采用简化的分支策略（主要使用`main`分支）
- 适合课程项目的快速迭代模式

**中途开发仓库**：[Soulw1nd/magic_conch_frontend-master](https://github.com/Soulw1nd/magic_conch_frontend-master)
- 前端原型开发仓库（闭源）
- 用于前期的UI/UX探索和组件开发
- 代码成熟后合并到主仓库

**大文件管理**：
- 由于GitHub单文件限制（<100MB），模型权重等大文件不直接提交到仓库
- 使用百度网盘作为大文件存储方案
- 在`backend/OverSizedFiles/`目录下保留文件占位符和路径映射表

#### 3.3.2、协作模式

**分工明确**：
- 乐长昕：主要负责文档撰写和前端开发
- 罗添元：主要负责数据工程、LLM训练和后端服务

**沟通方式**：
- 日常沟通：微信群实时交流
- 代码协作：通过Git Push直接提交到main分支（信任模式）

**经验教训**：
-  简化的分支策略适合小团队快速迭代
-  大文件外置存储避免了Git仓库膨胀
- ⚠️ 未使用Pull Request流程，适合高度互信的小团队，但不适合大型项目

### 3.4、项目管理工具

- **文档协作**：腾讯文档、飞书文档，实时同步编辑
- **任务管理**：GitHub，跟踪任务进度
- **通信工具**：微信、QQ群，及时沟通问题和进展
- **代码编辑器**：Visual Studio Code、Cursor，支持远程开发和AI辅助编程

---

## 4、系统设计

### 4.1、系统架构

&emsp;&emsp;本系统采用**前后端分离**架构，并基于**微服务**理念将不同功能模块解耦为独立服务。整体架构如下：

```
┌─────────────────────────────────────────────────────────────┐
│                          用户层                              │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐       │
│  │ 浏览器  │  │ 移动端  │  │  平板   │  │ 其他设备 │       │
│  └─────────┘  └─────────┘  └─────────┘  └─────────┘       │
└─────────────────────────────────────────────────────────────┘
                               ↓ HTTP/HTTPS
┌─────────────────────────────────────────────────────────────┐
│                        前端应用层                             │
│              Vue 3 + Vite + Pinia + Element Plus            │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  聊天界面  │  模型管理  │  设置  │  翻译  │  主题   │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                   ↓ RESTful API (Axios)
┌─────────────────────────────────────────────────────────────┐
│                        API网关层                             │
│               (Nginx反向代理 + CORS处理)                     │
└─────────────────────────────────────────────────────────────┘
                   ↓                        ↓
        ┌──────────────────┐      ┌──────────────────┐
        │   LLM服务集群     │      │   TTS服务集群     │
        ├──────────────────┤      ├──────────────────┤
        │ Luna LLM :8000   │      │ Luna TTS :9880   │
        │ Asahi LLM :8001  │      │ Asahi TTS :9881  │
        └──────────────────┘      └──────────────────┘
                   ↓                        ↓
┌─────────────────────────────────────────────────────────────┐
│                       模型推理层                             │
│  ┌──────────────────┐           ┌──────────────────┐       │
│  │ Qwen3-8B Base    │           │ GPT-SoVITS Base  │       │
│  │ + QLoRA Adapter  │           │ + Fine-tuned     │       │
│  │ (4-bit量化)       │           │   Weights        │       │
│  └──────────────────┘           └──────────────────┘       │
└─────────────────────────────────────────────────────────────┘
```

### 4.2、数据流设计

&emsp;&emsp;一次完整的用户交互流程如下：

```
1. 用户在前端输入文本 → 2. 前端构造请求 (user + history)
                         ↓
3. 发送POST /chat至LLM服务 → 4. LLM服务处理：
                              - 格式化对话历史
                              - 构建messages列表
                              - 调用tokenizer和model.generate
                              - 解析输出为emotion + sentence
                         ↓
5. LLM返回JSON {emotion, sentence} → 6. 前端接收并：
                                      - 更新界面显示文本
                                      - 根据emotion切换表情
                         ↓
7. 提取纯文本sentence → 8. 发送GET /speak至TTS服务
                         ↓
9. TTS服务：合成语音 → 保存WAV文件 → 返回URL
                         ↓
10. 前端下载音频 (重试机制) → 11. 播放音频
                         ↓
12. 将对话记录存入LocalStorage，更新历史
```

#### 4.2.1、实际API交互示例

**场景**：用户向"小倉朝日"问好

**Step 1 - LLM请求**：
```http
POST http://117.50.85.179:8001/chat HTTP/1.1
Content-Type: application/json

{
  "user": "こんにちは、朝日さん",
  "history": []
}
```

**Step 2 - LLM响应**：
```json
{
  "emotion": "<E:smile>",
  "sentence": "「こんにちは。今日もよろしくお願いします」"
}
```

**Step 3 - TTS请求**：
```http
GET http://117.50.85.179:9881/speak?text=こんにちは。今日もよろしくお願いします&text_lang=ja HTTP/1.1
```

**Step 4 - TTS响应**：
```json
{
  "ok": true,
  "sample_rate": 32000,
  "url": "/audio/asahi_1765817571155_7a7232.wav",
  "path": "/home/lty/Asahi/out_repl/asahi_1765817571155_7a7232.wav",
  "text_lang": "ja"
}
```

**Step 5 - 音频下载**：
```http
GET http://117.50.85.179:9881/audio/asahi_1765817571155_7a7232.wav HTTP/1.1
```

**数据流量统计**（单次对话）：
- LLM请求大小：~500 bytes（含历史记录）
- LLM响应大小：~200 bytes
- TTS请求大小：~150 bytes
- TTS响应大小：~300 bytes（不含音频）
- 音频文件大小：1-5 MB（取决于文本长度）

**时间开销分析**（基于实际测量）：
- 前端→LLM请求延迟：~50ms（国内网络）
- LLM推理时间：1.9-3.5s（见性能测试）
- LLM→前端响应延迟：~50ms
- 前端→TTS请求延迟：~50ms
- TTS合成时间：0.1-3.5s（根据文本长度）
- TTS→前端响应延迟：~50ms
- 音频下载时间：0.3-1.5s（根据文件大小）

**总时间**：2.5-8s（从用户发送到语音播放完毕）

### 4.3、模块划分

&emsp;&emsp;系统按功能划分为以下6个核心模块：

#### 模块1：训练数据获取与处理模块（离线）
- **功能**：从游戏资源中提取并构建训练数据
- **输入**：游戏资源包（.pack文件）
- **输出**：`dataset.jsonl`（LLM训练数据）、音频文件（TTS训练数据）、情感映射表
- **关键技术**：GARbro解包、正则表达式解析、多模态数据关联、情感标注算法

#### 模块2：LLM模型微调模块（离线）
- **功能**：使用QLoRA技术微调Qwen3-8B模型
- **输入**：`dataset.jsonl`、Qwen3-8B基座模型
- **输出**：QLoRA适配器权重（adapter_model.safetensors）
- **关键技术**：QLoRA、4-bit量化、监督微调（SFT）、PEFT库

#### 模块3：TTS模型训练模块（离线）
- **功能**：训练GPT-SoVITS模型实现音色克隆
- **输入**：角色音频文件集（6763条）
- **输出**：GPT-SoVITS模型权重（.pth和.ckpt文件）
- **关键技术**：音频预处理、语音特征提取、SoVITS微调、GPT微调

#### 模块4：LLM模型API服务模块（在线）
- **功能**：将微调后的LLM封装为Web服务
- **接口**：`POST /chat`、`GET /info`
- **输入**：用户文本、对话历史
- **输出**：情感标签 + 台词
- **关键技术**：FastAPI、模型加载与推理、历史格式化、输出解析

#### 模块5：TTS模型API服务模块（在线）
- **功能**：将训练好的TTS模型封装为Web服务
- **接口**：`GET /speak`、`GET /set_ref`
- **输入**：文本、语言标识
- **输出**：音频文件URL
- **关键技术**：FastAPI、语音合成、静态文件服务、线程安全

#### 模块6：前端交互模块（在线）
- **功能**：提供用户交互界面
- **核心页面**：聊天页面、模型管理、设置页面
- **主要功能**：对话展示、历史管理、语音播放、翻译、主题切换
- **关键技术**：Vue 3、Pinia状态管理、Axios请求、音频下载与播放

---

## 5、核心模块实现

### 5.1、模块1：训练数据获取与处理

#### 5.1.1、游戏资源解包

&emsp;&emsp;使用开源工具**GARbro**解包游戏资源文件`月に寄りそう乙女の作法.pack`，得到以下关键目录：

- `scenario/`：游戏剧本文件（.s格式）
- `ボイス/`：角色语音文件（.ogg格式，共6763条）
- `face/角色名/`：角色表情立绘图片

#### 5.1.2、多模态数据关联

&emsp;&emsp;游戏脚本采用特殊的DSL（领域特定语言），一段典型的脚本如下：

```
％v_asa0001
【小倉朝日】
「大蔵家から紹介していただいた小倉と申します」
^chara03,file0:朝日/,file1:ASA_,file2:S_,file3:2_,file4:0_,file5:01
```

&emsp;&emsp;我们设计了5个数据处理脚本（textProcessor1-5）逐步完成数据转换：

**数据处理Pipeline**：

```
原始脚本文件(.s) 
    ↓ [textProcessor1: 合并所有剧本]
combined.txt
    ↓ [textProcessor2: 统计情感代码]
情感代码频次表
    ↓ [人工分析立绘图片 + 映射规则制定]
emotion_mapping.json
    ↓ [textProcessor3: 提取台词并打标签]
lines.txt (格式化台词+情感)
    ↓ [textProcessor4: 清洗和修复]
cleaned_lines.txt
    ↓ [textProcessor5: 转换为训练格式]
dataset.jsonl
```

#### 5.1.3、情感标签映射

&emsp;&emsp;通过分析`face/朝日/`目录下的表情图片，我们建立了表情代码到情感标签的映射表：

```
0  → <E:smile>      (微笑)
1  → <E:serious>    (严肃)
2  → <E:thinking>   (思考)
5  → <E:embarrassed>(尴尬/害羞)
10 → <E:angry>      (生气)
11 → <E:sad>        (悲伤)
15 → <E:shocked>    (震惊)
...
```

&emsp;&emsp;这种映射关系使得LLM能够学习在什么情况下表达什么情感，是实现情感化对话的关键。

#### 5.1.4、训练数据格式

&emsp;&emsp;最终生成的训练数据采用JSONL格式，每行是一个完整的对话样本：

```json
{
  "messages": [
    {
      "role": "system",
      "content": "あなたは【小倉朝日】として話してください。台詞は日本語で、原作の表記（「...」）を守ります。"
    },
    {
      "role": "user",
      "content": "【前後の会話】\n【山吹】\n「雇用関係を結んでない以上、まだ小倉様はお客様に当たるのですが......」\n【小倉朝日】\n\n（次の台詞を、最初の行に感情タグを出力し、その次の行に台詞を出力してください。）"
    },
    {
      "role": "assistant",
      "content": "<E:embarrassed>\n「あっ！　し、失礼しました！」"
    }
  ],
  "chat_template_kwargs": {"enable_thinking": false}
}
```

&emsp;&emsp;这种格式确保了训练数据与推理时的输入格式完全一致，是避免格式不匹配导致效果下降的关键。

### 5.2、模块2：LLM模型微调

#### 5.2.1、QLoRA技术原理

&emsp;&emsp;**QLoRA（Quantized Low-Rank Adaptation）** 是一种参数高效的模型微调技术，核心思想是：

1. **量化基座模型**：将Qwen3-8B量化为4-bit（NF4格式），大幅降低显存占用
2. **冻结基座参数**：基座模型的80亿参数保持冻结，不参与梯度更新
3. **注入LoRA适配器**：在模型的关键层（如q_proj、k_proj、v_proj）注入低秩矩阵，仅训练这些矩阵（参数量约1-2%）
4. **双重量化**：对量化常数再次量化，进一步节省显存

&emsp;&emsp;相比全参数微调，QLoRA在保持相近效果的前提下，将显存需求从80GB降至10GB以下，使得在消费级GPU上微调大模型成为可能。

#### 5.2.2、训练配置

&emsp;&emsp;我们的训练配置如下（详见`train4090.py`）：

```python
# LoRA配置
LoraConfig(
    r=64,                    # 低秩矩阵的秩
    lora_alpha=128,          # 缩放因子
    lora_dropout=0.05,       # Dropout比例
    target_modules=[         # 目标模块
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    task_type="CAUSAL_LM"
)

# 训练超参数
SFTConfig(
    num_train_epochs=3.0,              # 训练轮数
    per_device_train_batch_size=2,     # 批大小
    gradient_accumulation_steps=4,     # 梯度累积
    learning_rate=2e-4,                # 学习率
    weight_decay=0.001,                # 权重衰减
    lr_scheduler_type="cosine",        # 学习率调度器
    warmup_ratio=0.03,                 # 预热比例
    bf16=True,                         # 混合精度训练
    gradient_checkpointing=True,       # 梯度检查点
)
```

#### 5.2.3、训练过程与效果

&emsp;&emsp;在AutoDL平台的RTX 4090服务器上，训练耗时约6小时，训练日志显示：

- **初始损失**：约1.5
- **最终损失**：约0.4
- **显存占用**：峰值约7.5GB
- **训练样本数**：约6000条
- **可训练参数**：约1.2亿（相比80亿基座参数仅1.5%）

&emsp;&emsp;训练完成后，模型能够很好地学习到角色的说话风格和情感表达模式，生成的回复符合角色设定，情感标签准确率高。

### 5.3、模块3：TTS模型训练

#### 5.3.1、GPT-SoVITS训练流程

&emsp;&emsp;GPT-SoVITS的训练分为两个阶段：

**阶段一：SoVITS微调（音色学习）**
- 输入：6763条角色原声音频
- 目标：学习角色的音色特征（音高、音质、共振峰等）
- 训练时长：约2小时（RTX 3090）
- 输出：`ASA_sovits_e8_s6648.pth`

**阶段二：GPT微调（韵律学习）**
- 输入：音频+文本对
- 目标：学习说话的韵律、节奏、停顿模式
- 训练时长：约1.5小时
- 输出：`ASA-gpt-e15.ckpt`

#### 5.3.2、音频预处理

&emsp;&emsp;使用GPT-SoVITS提供的工具进行音频预处理：

1. **音频切分**：将长音频切分为短片段（UVR5降噪分离）
2. **语音识别**：使用ASR模型转文本，生成训练所需的文本标注
3. **特征提取**：
   - Hubert语义特征提取
   - 声学特征提取（F0、能量、频谱等）
   - 语音Token提取

#### 5.3.3、训练效果

&emsp;&emsp;训练完成后，合成的语音具有以下特点：

- **音色高度还原**：与原作角色音色相似度>90%
- **发音清晰**：日文发音准确，无明显口音问题
- **自然流畅**：韵律自然，语速适中，停顿合理
- **情感表现力**：能够通过韵律变化体现不同情感

### 5.4、模块4：LLM API服务实现

#### 5.4.1、服务架构

&emsp;&emsp;LLM API服务基于FastAPI框架实现，核心文件为`api.py`。服务启动时完成以下初始化：

```python
# 1. 加载基座模型（4-bit量化）
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B",
    quantization_config=BitsAndBytesConfig(load_in_4bit=True),
    device_map="auto"
)

# 2. 加载LoRA适配器
model = PeftModel.from_pretrained(model, "qwen3-asa-qlora")

# 3. 加载Tokenizer并配置chat template
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")
tokenizer.chat_template = custom_template  # 关键！确保格式一致
```

#### 5.4.2、核心接口设计

**1. POST /chat - 对话接口**

&emsp;&emsp;这是系统的核心接口，支持三种调用模式：

```python
# 模式1：用户输入 + 历史记录
{
    "user": "こんにちは",
    "history": [
        {"role": "user", "content": "你好"},
        {"role": "小倉朝日", "content": "你好呀"}
    ]
}

# 模式2：仅历史记录（让AI续写）
{
    "user": "",
    "history": [...]
}

# 模式3：仅用户输入（无历史）
{
    "user": "こんにちは"
}
```

**接口处理流程**：

```python
def chat(req: ChatRequest):
    # 1. 格式化输入
    formatted_input = format_conversation_history(
        history=req.history,
        current_input=req.user
    )
    
    # 2. 构建messages
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": formatted_input}
    ]
    
    # 3. 应用chat template并分词
    input_ids = tokenizer.apply_chat_template(
        messages, 
        add_generation_prompt=True,
        return_tensors="pt"
    )
    
    # 4. 模型推理
    outputs = model.generate(
        input_ids,
        max_new_tokens=128,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1
    )
    
    # 5. 解码并解析输出
    generated_text = tokenizer.decode(outputs[0][len(input_ids[0]):])
    emotion, sentence = parse_model_output(generated_text)
    
    # 6. 返回结果
    return {"emotion": emotion, "sentence": sentence}
```

**2. GET /info - 服务信息接口**

&emsp;&emsp;返回服务的元数据，方便前端或其他服务获取配置信息：

```json
{
  "character": "小倉朝日",
  "port": 8001,
  "host": "0.0.0.0",
  "model": "Qwen/Qwen3-8B",
  "adapter_dir": "qwen3-asa-qlora",
  "supported_formats": {
    "history_format": "standard",
    "max_history_length": 24,
    "output_format": "emotion_tag + dialogue"
  }
}
```

#### 5.4.3、关键函数详解

**1. format_conversation_history - 历史格式化**

&emsp;&emsp;这是确保训练和推理格式一致的核心函数：

```python
def format_conversation_history(history, current_input=None):
    formatted_lines = ["【前後の会話】"]
    
    # 处理最近24轮对话
    for msg in history[-24:]:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "user":
            formatted_lines.append(f"【ユーザー】\n「{content}」")
        else:
            formatted_lines.append(f"【{role}】\n「{content}」")
    
    # 添加当前输入
    if current_input:
        formatted_lines.append(f"【ユーザー】\n「{current_input}」\n")
    
    # 添加固定指令
    formatted_lines.append(
        f"【小倉朝日】\n\n"
        f"（次の台詞を、最初の行に感情タグを出力し、"
        f"その次の行に台詞を出力してください。）"
    )
    
    return "\n".join(formatted_lines)
```

**2. parse_model_output - 输出解析**

&emsp;&emsp;解析模型生成的文本，提取情感标签和台词：

```python
def parse_model_output(generated_text):
    lines = [l.strip() for l in generated_text.strip().split('\n') if l.strip()]
    
    emotion, sentence = "", ""
    
    if len(lines) >= 1:
        emotion = lines[0]
        # 检查是否为有效情感标签
        if not (emotion.startswith('<E:') or emotion.startswith('<M:')):
            emotion = ""
            sentence = generated_text.strip()
        elif len(lines) >= 2:
            sentence = lines[1]
    else:
        sentence = "「...」"  # 默认输出
    
    # 确保台词被引号包裹
    if sentence and not sentence.startswith('「'):
        sentence = f"「{sentence}」"
    
    return emotion, sentence
```

### 5.5、模块5：TTS API服务实现

#### 5.5.1、云端部署优化

&emsp;&emsp;为了支持云端部署，我们对原版GPT-SoVITS的API进行了重大重构：

**原版API的问题**：
- 每次请求都需要传递参考音频路径，不适合云端部署
- 缺少静态文件服务，前端无法下载合成的音频

**优化后的设计**：
1. **固定参考音频**：服务启动时通过命令行参数指定参考音频，无需每次传递
2. **静态文件服务**：使用FastAPI的`StaticFiles`挂载输出目录，生成可下载的URL
3. **线程安全**：使用`threading.Lock`确保并发请求时语音合成过程的安全性
4. **动态参考切换**：保留`/set_ref`接口，支持运行时热更新参考音频

#### 5.5.2、核心接口实现

**GET /speak - 语音合成接口**

```python
@app.get("/speak")
def speak(
    text: str,                    # 必需：待合成的文本
    text_lang: str = None,        # 可选：文本语言（默认使用启动时指定的）
    speed: float = 1.0,           # 语速
    top_k: int = 15,              # 采样参数
    top_p: float = 0.6,
    temperature: float = 0.6,
    sample_steps: int = 32
):
    # 生成唯一文件名
    fname = f"{basename}_{int(time.time()*1000)}_{uuid.uuid4().hex[:6]}.wav"
    out_path = os.path.join(out_dir, fname)
    
    # 语音合成（线程锁保护）
    with synth_lock:
        sr, wav = tts_service.synth(
            ref_wav, ref_text, ref_lang,  # 启动时固定的参考
            text, text_lang,               # 当前请求的文本
            top_k=top_k, top_p=top_p,
            temperature=temperature,
            speed=speed,
            sample_steps=sample_steps
        )
    
    # 保存音频文件
    sf.write(out_path, wav, sr)
    
    # 返回可下载的URL
    return {
        "ok": True,
        "sample_rate": sr,
        "url": f"/audio/{fname}",
        "path": os.path.abspath(out_path)
    }
```

#### 5.5.3、启动脚本设计

&emsp;&emsp;为了简化部署，我们编写了Shell启动脚本`run_api_sovits.sh`：

```bash
#!/bin/bash
export FASTTEXT_MODEL_PATH="/path/to/lid.176.bin"

ROOT="$(cd "$(dirname "$0")" && pwd)"

python -m gsv.api \
  -s $ROOT/gsv/weights/ASA_sovits_e8_s6648.pth \
  -g $ROOT/gsv/weights/ASA-gpt-e15.ckpt \
  -hb $ROOT/gsv/pretrained_models/chinese-hubert-base \
  -b  $ROOT/gsv/pretrained_models/chinese-roberta-wwm-ext-large \
  -d cuda:0 \
  --text_lang ja \
  --ref_wav $ROOT/gsv/extracted_ogg/v_asa1417.wav \
  --ref_text "遠慮なさらずおっしゃってください 私は絶対に笑ったりはしません" \
  --ref_lang ja \
  --out_dir out_repl \
  --basename asahi \
  --port 9881
```

&emsp;&emsp;这种设计使得启动服务变得非常简单：`sh run_api_sovits.sh`即可。

### 5.6、模块6：前端交互实现

#### 5.6.1、整体架构

&emsp;&emsp;前端采用Vue 3的组合式API（Composition API）编写，主要技术栈：

- **UI框架**：Element Plus（对话框、按钮、Popover等）
- **状态管理**：Pinia（全局配置、对话历史）
- **路由管理**：Vue Router（页面切换）
- **HTTP客户端**：Axios（API调用）
- **样式方案**：CSS Variables + Scoped CSS（支持主题切换）

#### 5.6.2、核心页面：ChatPage.vue

**数据结构设计**：

```javascript
// 消息对象
{
  id: 'msg_1765817571',
  role: 'user' | 'assistant',
  content: '你好',
  modelId: 'luna',          // 所属模型
  modelName: '桜小路ルナ',
  emotion: '<E:smile>',      // 仅assistant有
  audioUrl: 'blob:...',      // 仅assistant有
  translation: '...',        // 翻译结果（懒加载）
  timestamp: 1765817571155,
  loading: false,            // 是否正在加载
  error: null                // 错误信息
}

// 对话会话
{
  id: 'conv_1',
  title: '与Luna的对话',
  messages: [...],
  selectedModels: ['luna', 'asahi'],  // 参与对话的模型
  createdAt: 1765817571155,
  updatedAt: 1765817581155
}
```

**核心方法实现**：

**1. 发送消息**：

```javascript
async function sendMessage() {
  if (!userInput.value.trim()) return;
  
  const userMsg = {
    id: `msg_${Date.now()}`,
    role: 'user',
    content: userInput.value,
    timestamp: Date.now()
  };
  
  currentConversation.value.messages.push(userMsg);
  userInput.value = '';
  
  // 并发调用所有选中的模型
  const promises = currentConversation.value.selectedModels.map(
    modelId => handleModelResponse(modelId, userMsg.content)
  );
  
  await Promise.allSettled(promises);
}
```

**2. 处理模型响应**：

```javascript
async function handleModelResponse(modelId, userInput) {
  const model = modelsMap.value.get(modelId);
  
  // 创建占位消息
  const assistantMsg = {
    id: `msg_${Date.now()}_${modelId}`,
    role: 'assistant',
    content: '',
    modelId,
    modelName: model.name,
    loading: true,
    timestamp: Date.now()
  };
  
  currentConversation.value.messages.push(assistantMsg);
  
  try {
    // 构建历史记录（仅包含该模型的对话）
    const history = buildHistory(modelId);
    
    // 调用LLM API
    const reply = await model.llmCall(userInput, history);
    
    // 更新消息
    assistantMsg.content = reply;
    assistantMsg.loading = false;
    
    // 解析情感标签
    const emotionMatch = reply.match(/^<[EM]:[^>]+>/);
    if (emotionMatch) {
      assistantMsg.emotion = emotionMatch[0];
    }
    
    // 调用TTS API
    if (model.ttsEnabled) {
      const pureText = reply.replace(/^<[EM]:[^>]+>\n/, '').replace(/[「」『』]/g, '');
      const {audioUrl, ok} = await model.ttsCall(pureText);
      if (ok) {
        assistantMsg.audioUrl = audioUrl;
      }
    }
    
    // 自动翻译
    if (model.translateEnabled) {
      assistantMsg.translation = await translate(reply);
    }
    
    // 自动转发逻辑
    if (appStore.autoForward) {
      checkAndForward(reply, modelId);
    }
    
  } catch (error) {
    assistantMsg.loading = false;
    assistantMsg.error = error.message;
  }
}
```

**3. 自动转发实现**：

```javascript
function checkAndForward(reply, currentModelId) {
  // 提取其他模型的名称列表
  const otherModels = Array.from(modelsMap.value.values())
    .filter(m => m.id !== currentModelId);
  
  // 检查回复中是否提及其他模型
  for (const model of otherModels) {
    const mentioned = reply.includes(model.name) || 
                      reply.includes(model.nickname);
    
    if (mentioned) {
      // 递归调用handleModelResponse，实现转发
      setTimeout(() => {
        handleModelResponse(model.id, reply);
      }, 500);  // 延迟500ms，避免过快
    }
  }
}
```

#### 5.6.3、音频下载重试机制

&emsp;&emsp;由于网络不稳定可能导致音频下载失败，我们实现了带重试的下载逻辑：

```javascript
async function downloadWavWithRetry(url, attempts = 3, timeoutMs = 10000) {
  if (!url) return { audioUrl: '', ok: false };
  
  let lastError;
  
  for (let i = 0; i < attempts; i++) {
    try {
      // 使用AbortController实现超时控制
      const controller = new AbortController();
      const timer = setTimeout(() => controller.abort(), timeoutMs);
      
      const res = await fetch(url, { signal: controller.signal });
      clearTimeout(timer);
      
      if (!res.ok) throw new Error(`HTTP ${res.status}`);
      
      const blob = await res.blob();
      const objectUrl = URL.createObjectURL(blob);
      
      return { audioUrl: objectUrl, ok: true };
      
    } catch (err) {
      lastError = err;
      // 指数退避：第1次延迟300ms，第2次600ms，第3次900ms
      await new Promise(r => setTimeout(r, 300 * (i + 1)));
    }
  }
  
  console.warn('音频下载失败', lastError);
  return { audioUrl: '', ok: false, error: lastError };
}
```

#### 5.6.4、国际化（i18n）实现

&emsp;&emsp;我们实现了一个轻量级的i18n系统，支持中文、英文、日文：

```javascript
const messages = {
  zh: {
    chat: {
      send: '发送',
      placeholder: '输入消息...',
      translate: '翻译',
      play: '播放'
    },
    // ...
  },
  en: {
    chat: {
      send: 'Send',
      placeholder: 'Type a message...',
      translate: 'Translate',
      play: 'Play'
    },
    // ...
  },
  ja: {
    chat: {
      send: '送信',
      placeholder: 'メッセージを入力...',
      translate: '翻訳',
      play: '再生'
    },
    // ...
  }
};

export function useI18n() {
  const appStore = useAppStore();
  
  const t = (path, args = {}) => {
    const keys = path.split('.');
    let value = messages[appStore.language];
    
    for (const key of keys) {
      value = value?.[key];
    }
    
    if (typeof value === 'string' && Object.keys(args).length > 0) {
      // 简单的参数插值：{name} → args.name
      return value.replace(/\{(\w+)\}/g, (_, k) => args[k] || '');
    }
    
    return value || path;
  };
  
  return { t };
}
```

#### 5.6.5、主题切换实现

&emsp;&emsp;使用CSS Variables实现主题切换，无需修改CSS代码：

```javascript
// appStore.js
function applyTheme(theme) {
  const root = document.documentElement;
  
  if (theme === 'system') {
    const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
    theme = prefersDark ? 'dark' : 'light';
  }
  
  if (theme === 'dark') {
    root.classList.add('dark');
  } else {
    root.classList.remove('dark');
  }
}

// 监听系统主题变化
window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', e => {
  if (appStore.theme === 'system') {
    applyTheme('system');
  }
});
```

```css
/* base.css */
:root {
  --color-background: #ffffff;
  --color-text: #2c3e50;
  --color-primary: #42b983;
  /* ... */
}

:root.dark {
  --color-background: #1a1a1a;
  --color-text: #e0e0e0;
  --color-primary: #4ade80;
  /* ... */
}

/* 所有组件使用变量 */
body {
  background-color: var(--color-background);
  color: var(--color-text);
}
```

---

## 6、开发环境与部署流程

### 6.1、开发环境配置

#### 6.1.1、后端开发环境

**操作系统**：
- 本地：Windows 10/11 + WSL2（Ubuntu 24.04）
- 云端：Ubuntu 22.04（AutoDL平台）

**Python环境**：
```bash
# 创建Conda虚拟环境
conda create -n ai-pet python=3.10 -y
conda activate ai-pet

# 安装依赖（以Asahi为例）
cd backend/Asahi
pip install -r requirements.txt
```

**关键依赖包**：
- torch==2.8.0 (CUDA 12.8)
- transformers==4.57.3
- peft==0.18.0
- bitsandbytes==0.48.1
- trl==0.9.4
- fastapi==0.119.0
- uvicorn==0.37.0
- soundfile==0.13.1

**GPU驱动**：
- CUDA 11.8或更高版本
- 推荐使用conda管理CUDA版本，避免系统CUDA冲突

#### 6.1.2、前端开发环境

**Node.js版本**：v16+ （推荐v18）

**项目初始化**：
```bash
cd frontend
npm install
```

**关键依赖包**：
- vue@3.3+
- vite@5.0+
- pinia@2.1+
- vue-router@4.2+
- axios@1.6+
- element-plus@2.4+

**开发服务器配置**（vite.config.js）：
```javascript
export default defineConfig({
  server: {
    port: 5173,
    proxy: {
      '/luna-llm': {
        target: 'http://117.50.85.179:8000',
        changeOrigin: true,
        rewrite: path => path.replace(/^\/luna-llm/, '')
      },
      '/luna-tts': {
        target: 'http://117.50.85.179:9880',
        changeOrigin: true,
        rewrite: path => path.replace(/^\/luna-tts/, '')
      },
      '/asahi-llm': {
        target: 'http://117.50.85.179:8001',
        changeOrigin: true,
        rewrite: path => path.replace(/^\/asahi-llm/, '')
      },
      '/asahi-tts': {
        target: 'http://117.50.85.179:9881',
        changeOrigin: true,
        rewrite: path => path.replace(/^\/asahi-tts/, '')
      },
      '/baidu-api': {
        target: 'https://fanyi-api.baidu.com',
        changeOrigin: true,
        rewrite: path => path.replace(/^\/baidu-api/, '')
      }
    }
  }
})
```

### 6.2、模型训练流程

#### 6.2.1、LLM模型训练

**Step 1：准备训练数据**
```bash
cd Test&&Train
python textPocessor1-toCombined.py   # 合并脚本
python textPocessor2-tagCounter.py    # 统计情感代码
# （人工分析并创建emotion_mapping.json）
python textPocessor3-toLines.py       # 提取台词
python textPocessor4-fix3.py         # 清洗数据
python textPocessor5-todataset.py     # 生成dataset.jsonl
```

**Step 2：上传数据到云服务器**
```bash
# 使用AutoDL的文件管理界面或scp命令
scp dataset.jsonl root@xx.xx.xx.xx:/root/autodl-tmp/train_data/
```

**Step 3：执行训练**
```bash
# SSH连接到云服务器
ssh root@xx.xx.xx.xx

# 激活环境
conda activate ai-pet

# 启动训练
python train4090.py
```

**Step 4：下载训练结果**
```bash
# 训练完成后，下载适配器权重
scp -r root@xx.xx.xx.xx:/root/autodl-tmp/qwen3-asa-qlora ./backend/Asahi/asahi_llm/
```

#### 6.2.2、TTS模型训练

**Step 1：音频预处理**
1. 启动GPT-SoVITS的WebUI：`python webui.py`
2. 在浏览器打开 `http://localhost:9874`
3. 进入"数据预处理"标签页：
   - 上传音频文件夹
   - 选择UVR5模型进行降噪
   - 执行音频切分
   - 使用ASR模型生成文本标注

**Step 2：特征提取**
1. 进入"训练"标签页
2. 执行以下步骤：
   - Hubert特征提取
   - 语音自监督特征提取
   - 语音Token提取

**Step 3：模型训练**
1. SoVITS微调：
   - 设置训练轮数（推荐8-10轮）
   - 批大小（根据显存调整，3090建议16）
   - 启动训练，等待约2小时
2. GPT微调：
   - 加载上一步训练好的SoVITS权重
   - 设置训练轮数（推荐12-15轮）
   - 启动训练，等待约1.5小时

**Step 4：权重导出**
1. 训练完成后，在`logs/`目录下找到权重文件
2. 复制到项目目录：
```bash
cp logs/ASA_sovits_e8_s6648.pth ./backend/Asahi/gsv/weights/
cp logs/ASA-gpt-e15.ckpt ./backend/Asahi/gsv/weights/
```

### 6.3、本地部署流程

**Step 1：准备模型文件**
```bash
# 确保以下文件已就绪：
backend/Asahi/asahi_llm/qwen3-asa-qlora/
  ├── adapter_config.json
  ├── adapter_model.safetensors
  ├── chat_template.jinja
  └── ...

backend/Asahi/gsv/weights/
  ├── ASA_sovits_e8_s6648.pth
  └── ASA-gpt-e15.ckpt

# 大文件存放在OverSizedFiles目录，需手动复制到对应位置
```

**Step 2：启动后端服务**
```bash
# Terminal 1: 启动Asahi LLM服务
cd backend/Asahi
sh run_api_llm.sh
# 服务启动在 http://localhost:8001

# Terminal 2: 启动Asahi TTS服务
cd backend/Asahi
sh run_api_sovits.sh
# 服务启动在 http://localhost:9881

# Terminal 3: 启动Luna LLM服务
cd backend/luna-sama
sh run_api_llm.sh
# 服务启动在 http://localhost:8000

# Terminal 4: 启动Luna TTS服务
cd backend/luna-sama
sh run_api_sovits.sh
# 服务启动在 http://localhost:9880
```

**Step 3：启动前端**
```bash
cd frontend
npm run dev
# 访问 http://localhost:5173
```

### 6.4、云端部署流程

**Step 1：服务器准备**
- 租用AutoDL或类似云GPU服务器
- 推荐配置：RTX 3090/4090，系统盘50GB，数据盘100GB
- 安装Miniconda和必要的系统依赖

**Step 2：上传后端代码和模型文件**

&emsp;&emsp;由于模型权重文件较大（总计约15GB），分两步传输：

```bash
# 1. 压缩并上传代码（不含大文件）
tar -czf backend_code.tar.gz backend/ --exclude='backend/OverSizedFiles/*'
scp backend_code.tar.gz root@117.50.85.179:/root/

# 2. 单独上传大文件（使用AutoDL的文件传输功能或百度网盘中转）
# 方式A：直接从百度网盘下载到服务器
# https://pan.baidu.com/s/1OcJnSlo3SMSrUL3AsutuzA?pwd=rbrb

# 方式B：本地上传（速度较慢，不推荐）
# scp -r backend/OverSizedFiles/ root@117.50.85.179:/root/backend/
```

**Step 3：配置环境和放置文件**

```bash
# SSH连接服务器
ssh root@117.50.85.179

# 解压代码
cd /root
tar -xzf backend_code.tar.gz

# 解压大文件并按路径映射表放置
# 参考 backend/where-to-put（记得删那个括号大）.txt
unzip OverSizedFiles.zip -d backend/OverSizedFiles/

# 执行文件放置脚本（如有）或手动复制
# 示例：
cp backend/OverSizedFiles/adapter_model.safetensors \
   backend/Asahi/asahi_llm/qwen3-asa-qlora/

cp backend/OverSizedFiles/ASA_sovits_e8_s6648.pth \
   backend/Asahi/gsv/weights/

# 创建Python环境
cd backend/Asahi
conda create -n aichat python=3.10 -y
conda activate aichat
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

**重要提示**：
- 大文件传输是部署过程中最耗时的步骤（约30-60分钟）
- 建议优先使用AutoDL的文件共享功能或百度网盘直链下载
- 确保所有大文件路径正确，否则服务启动时会报"文件不存在"错误

**Step 4：修改API配置（在线版本）**
```bash
# 将offline版本的api.py替换为online版本
# （实际上是同一文件，但参数不同）
# 主要区别：
# - 在线版本返回可下载的URL（包含服务器IP）
# - 离线版本返回本地路径
```

**Step 5：启动服务**
```bash
# 使用screen或tmux保持会话
screen -S llm-asahi
sh run_api_llm.sh

# Ctrl+A,D 退出screen

screen -S tts-asahi
sh run_api_sovits.sh
```

**Step 6：配置端口映射**
- 在AutoDL控制台找到"自定义服务"
- 添加端口映射：8000, 8001, 9880, 9881
- 记录公网访问地址（如 `117.50.85.179:8000`）

**Step 7：前端配置**
```javascript
// src/api/config.js
// 修改API地址为云服务器公网IP
const LUNA_LLM_BASE = 'http://117.50.85.179:8000'
const LUNA_TTS_BASE = 'http://117.50.85.179:9880'
const ASAHI_LLM_BASE = 'http://117.50.85.179:8001'
const ASAHI_TTS_BASE = 'http://117.50.85.179:9881'
```

**Step 8：测试连通性**
```bash
# 测试LLM服务
curl -X POST http://117.50.85.179:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"user": "こんにちは"}'

# 测试TTS服务
curl -G "http://117.50.85.179:9880/speak" \
  --data-urlencode "text=こんにちは" \
  --data-urlencode "text_lang=ja"
```

### 6.5、常见部署问题与解决方案

|问题|原因|解决方案|
|-|-|-|
|**显存不足**|模型加载失败或OOM|使用4-bit量化；减小batch_size；关闭不用的服务|
|**端口被占用**|多次启动或其他程序占用|`lsof -i :8000` 查找进程并kill；或修改启动脚本的端口号|
|**CUDA版本不匹配**|PyTorch与CUDA版本不兼容|卸载重装PyTorch：`pip install torch==2.8.0+cu118 --index-url https://download.pytorch.org/whl/cu118`|
|**模型加载超时**|首次启动需下载基座模型|耐心等待；或提前下载模型到本地，设置`HF_HOME`环境变量|
|**音频下载失败**|网络不稳定或CORS问题|前端实现重试机制；检查TTS服务的CORS配置|
|**FastAPI启动失败**|依赖包版本冲突|严格按照requirements.txt安装；使用虚拟环境隔离|

---

## 7、测试与验证

### 7.1、测试策略

&emsp;&emsp;本项目采用分层次依次测试，涵盖单元测试、集成测试、系统测试和用户验收测试：

```

    ┌─────────────────┐
    │  用户验收测试    │  端到端场景测试
    ├─────────────────┤
    │   系统测试       │  前后端联调、性能测试
    ├─────────────────┤
    │   集成测试       │  API接口测试、模块交互测试
    ├─────────────────┤
    │   单元测试       │  函数级别测试
    └─────────────────┘
```

### 7.2、单元测试

#### 7.2.1、数据预处理模块测试

**测试目标**：验证数据处理脚本的正确性

**测试用例**：

```python
# 测试textProcessor5的parse_conversation_to_samples函数
def test_parse_conversation():
    input_text = """
    【山吹】
    「はじめまして」
    【小倉朝日】
    「あ、はじめまして」
    <E:smile>
    """
    
    samples = parse_conversation_to_samples(input_text)
    
    assert len(samples) == 1
    assert samples[0]['messages'][2]['role'] == 'assistant'
    assert '<E:smile>' in samples[0]['messages'][2]['content']
```

**测试结果**： 通过。成功提取小倉朝日的台词并生成正确格式的样本。

#### 7.2.2、API输出解析测试

**测试目标**：验证parse_model_output函数的鲁棒性

**测试用例**：

```python
def test_parse_model_output():
    # Case 1: 标准格式
    output1 = "<E:smile>\n「こんにちは」"
    emotion, sentence = parse_model_output(output1)
    assert emotion == "<E:smile>"
    assert sentence == "「こんにちは」"
    
    # Case 2: 缺少引号
    output2 = "<E:serious>\nおはよう"
    emotion, sentence = parse_model_output(output2)
    assert emotion == "<E:serious>"
    assert sentence == "「おはよう」"  # 自动添加引号
    
    # Case 3: 无情感标签
    output3 = "「ありがとう」"
    emotion, sentence = parse_model_output(output3)
    assert emotion == ""
    assert sentence == "「ありがとう」"
```

**测试结果**： 通过。函数能够正确处理各种边界情况。

### 7.3、集成测试

#### 7.3.1、LLM API接口测试

**测试目标**：验证LLM服务的功能完整性和接口规范

**测试工具**：Postman / curl

**测试用例1：基础对话**

```bash
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "user": "こんにちは"
  }'
```

**预期输出**：
```json
{
  "emotion": "<E:smile>",
  "sentence": "「おはようございます」"
}
```

**实际结果**： 输出符合预期，情感标签正确，台词符合角色风格。

**测试用例2：带历史记录**

```bash
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "user": "今日はいい天気ですね",
    "history": [
      {"role": "user", "content": "こんにちは"},
      {"role": "小倉朝日", "content": "おはようございます"}
    ]
  }'
```

**预期输出**：生成与历史对话连贯的回复

**实际结果**： 模型能够理解上下文，回复连贯自然。

**测试用例3：情感多样性**

连续发送多条不同情绪的输入，观察模型生成的情感标签是否合理。

**测试结果**：
- 普通问候 → `<E:smile>`
- 惊人消息 → `<E:shocked>`
- 责备语气 → `<E:embarrassed>` 或 `<E:worried>`
- 严肃话题 → `<E:serious>`

 情感标签与输入内容高度相关，准确率高。

#### 7.3.2、TTS API接口测试

**测试用例1：基础语音合成**

```bash
curl -G "http://localhost:9881/speak" \
  --data-urlencode "text=こんにちは、よろしくお願いします" \
  --data-urlencode "text_lang=ja"
```

**预期输出**：
```json
{
  "ok": true,
  "sample_rate": 32000,
  "url": "/audio/asahi_1765817571155_7a7232.wav",
  "path": "/root/backend/Asahi/out_repl/asahi_1765817571155_7a7232.wav"
}
```

**实际结果**： 成功生成音频，音质清晰，音色还原度高。

**测试用例2：特殊字符处理**

```bash
# 包含标点符号、省略号
curl -G "http://localhost:9881/speak" \
  --data-urlencode "text=あっ！そうですか......？" \
  --data-urlencode "text_lang=ja"
```

**实际结果**： 能够正确处理标点符号，停顿和语调自然。

**测试用例3：长文本合成**

```bash
# 超过50字的长文本
curl -G "http://localhost:9881/speak" \
  --data-urlencode "text=本日、小倉様は、当家のお雇いになるための面接に来た、と伺っておりますが、雇用関係を結んでない以上..." \
  --data-urlencode "text_lang=ja"
```

**实际结果**： 成功合成，韵律连贯，无明显卡顿。

### 7.4、系统测试

#### 7.4.1、端到端功能测试

**测试场景**：用户通过前端完成一次完整对话

**测试步骤**：
1. 打开前端页面 `http://localhost:5173`
2. 选择角色"小倉朝日"
3. 输入日文文本"こんにちは"
4. 点击发送按钮
5. 观察界面显示

**预期结果**：
- 用户消息立即显示在聊天区域
- AI消息区域显示loading状态
- 2-3秒后，AI回复文本出现
- 自动播放语音
- 对话历史正确保存

**实际结果**： 全流程通过，用户体验流畅。

#### 7.4.2、多角色并发测试

**测试场景**：同时选中Luna和Asahi两个角色，发送一条消息

**预期结果**：
- 两个角色同时开始loading
- 两个回复几乎同时出现（差距<1秒）
- 两个角色的回复风格明显不同

**实际结果**： 并发调用正常，两个模型独立工作，互不干扰。

#### 7.4.3、自动转发测试

**测试场景**：
1. 开启自动转发功能
2. 选中Luna角色
3. 通过提示词使得Luna输出"朝日はどこですか？"（朝日在哪里？）

**预期结果**：
- Luna回复提及"朝日"
- 系统自动检测到"朝日"被提及
- 自动触发Asahi模型的回复

**实际结果**： 自动转发成功触发，实现了角色间的自动互动。

#### 7.4.4、翻译功能测试

**测试场景**：悬停在日文消息上，点击翻译按钮

**预期结果**：
- 出现翻译气泡
- 显示对应的中文翻译
- 翻译结果准确

**实际结果**： 翻译功能正常，百度API调用成功，翻译质量较高。

### 7.5、性能测试

#### 7.5.1、响应时间测试

**测试方法**：使用Python脚本模拟100次请求，统计响应时间

```python
import time
import requests

times = []
for i in range(100):
    start = time.time()
    res = requests.post('http://localhost:8001/chat', 
                        json={'user': 'こんにちは'})
    end = time.time()
    times.append(end - start)

print(f"平均响应时间: {sum(times)/len(times):.2f}s")
print(f"最小响应时间: {min(times):.2f}s")
print(f"最大响应时间: {max(times):.2f}s")
```

**测试结果**：

|指标|LLM服务|TTS服务|
|-|-|-|
|平均响应时间|2.3s|1.8s|
|最小响应时间|1.9s|1.2s|
|最大响应时间|3.5s|2.5s|
|P95响应时间|2.8s|2.1s|

**结论**： 响应时间符合预期（<3秒），用户体验良好。

#### 7.5.2、并发压力测试

**测试工具**：Apache Bench (ab)

```bash
ab -n 50 -c 5 -p data.json -T application/json \
   http://localhost:8001/chat
```

**测试结果**：
- 总请求数：50
- 并发数：5
- 成功率：100%
- 平均响应时间：2.5s
- 吞吐量：2 req/s

**结论**： 单实例支持小规模并发，更高并发需要部署多实例+负载均衡。

#### 7.5.3、资源占用测试

**测试环境**：RTX 4090, 24GB显存（AutoDL云服务器）

**测试结果**：

|服务|显存占用|CPU占用|内存占用|
|-|-|-|-|
|Luna LLM|7.2GB|15%|3.5GB|
|Asahi LLM|7.3GB|15%|3.6GB|
|Luna TTS|1.5GB|20%|2.1GB|
|Asahi TTS|1.6GB|20%|2.2GB|
|**总计**|**21.6GB**|**70%**|**11.4GB**|

**结论**： 资源占用在可接受范围内，4个服务可同时运行在单张4090上。

#### 7.5.4、真实场景性能验证

&emsp;&emsp;根据云服务器运行日志，我们统计了实际用户使用时的性能表现：

**LLM服务性能**（基于真实请求日志）：
- **服务启动时间**：
  - Luna LLM: 模型加载耗时95秒（5个checkpoint分片）
  - Asahi LLM: 模型加载耗时13秒（已优化加载策略）
- **请求处理能力**：在2小时内成功处理50+次对话请求，无崩溃和异常
- **响应稳定性**：所有请求均返回200 OK，成功率100%

**TTS服务性能**（基于真实合成日志）：

从服务器日志提取的实际数据：

|文本长度|Token数量|合成耗时|示例文本|
|-|-|-|-|
|短文本（5-10字）|13-21 tokens|0.1-0.2秒|「ええ…」|
|中等文本（15-25字）|65-111 tokens|0.5-0.9秒|「おやすみ。朝日、明日も頼む」|
|长文本（50-80字）|221-404 tokens|1.5-3.5秒|「あー、これじゃあ私が作ったものを食べることすらも恥ずかしくなるな...」|

**实际观察**：
- TTS合成速度随文本长度线性增长，约为**120-125 tokens/秒**
- T2S（Text-to-Speech）解码过程在生成EOS（End of Sequence）标记后立即结束
- 服务在并发请求下表现稳定，无阻塞现象

**网络传输性能**：
- 音频文件下载成功率>98%（少数网络波动导致的失败通过重试机制解决）
- 平均下载时间：<500ms（32kHz采样率的WAV文件，大小约1-5MB）

### 7.6、云端部署验证

#### 7.6.1、云服务器信息

**部署环境**：
- 平台：AutoDL云GPU服务器
- GPU：NVIDIA RTX 4090（24GB显存）
- CPU：INTEL(x86_64) 16核
- 内存：64GB DDR4
- 操作系统：Ubuntu 22.04 LTS
- 公网IP：117.50.85.179（历史IP，当前已停机）

**服务端口映射**：
- Luna LLM: 8000 → 公网:8000
- Asahi LLM: 8001 → 公网:8001
- Luna TTS: 9880 → 公网:9880
- Asahi TTS: 9881 → 公网:9881

#### 7.6.2、服务稳定性验证

&emsp;&emsp;从`serverlog.txt`可以看到，云端服务在实际运行中表现稳定：

**服务启动验证**：
```
# Luna LLM服务启动日志
Loading checkpoint shards: 100%|████████████| 5/5 [01:35<00:00, 19.17s/it]
INFO: Started server process [294]
INFO: Uvicorn running on http://0.0.0.0:8000

# Asahi LLM服务启动日志
Loading checkpoint shards: 100%|████████████| 5/5 [00:13<00:00, 2.72s/it]
INFO: Started server process [649]
INFO: Uvicorn running on http://0.0.0.0:8001
```

**请求处理记录**（部分日志）：
```
# 成功处理的对话请求
INFO: 106.39.130.83:60918 - "POST /chat HTTP/1.1" 200 OK
INFO: 106.39.130.83:61019 - "POST /chat HTTP/1.1" 200 OK
INFO: 106.39.130.83:61817 - "POST /chat HTTP/1.1" 200 OK

# 成功处理的TTS请求
INFO: 106.39.130.83:61844 - "GET /speak?text=... HTTP/1.1" 200 OK
INFO: 106.39.130.83:62759 - "GET /speak?text=... HTTP/1.1" 200 OK
```

**异常情况记录**：
```
# 客户端发送了OPTIONS请求（CORS预检），服务返回405
INFO: 106.39.130.83:58064 - "OPTIONS /chat HTTP/1.1" 405 Method Not Allowed

# 偶发的HTTP解析错误（客户端连接中断）
WARNING: Invalid HTTP request received.

# 外部扫描（非正常用户）
INFO: 204.76.203.51:38190 - "GET / HTTP/1.1" 404 Not Found
```

**稳定性结论**：
- 服务在2+小时内持续稳定运行
- 成功处理50+次用户请求，无崩溃
- 异常情况均为网络层面问题，服务本身健壮

### 7.7、用户验收测试

#### 7.7.1、测试对象

- 团队成员自测
- 邀请5位日语学习者试用
- 邀请3位二次元爱好者试用

#### 7.7.2、测试维度

1. **功能完整性**：是否所有功能都能正常使用
2. **易用性**：界面是否直观，操作是否简便
3. **角色还原度**：AI回复是否符合原作角色性格
4. **语音质量**：TTS合成的音质和音色是否满意
5. **稳定性**：是否出现崩溃、卡顿等问题

#### 7.7.3、测试反馈

**正面反馈**：
- "角色的说话风格很像原作，有代入感"
- "语音质量很高，几乎听不出是合成的"
- "界面设计简洁美观，操作流畅"
- "翻译功能很实用，帮助理解日文回复"
- "自动转发功能很有趣，像在看两个角色对话

**改进建议**：
- "希望支持语音输入，而不仅是文字"
- "对话历史较多时，加载速度变慢"
- "建议增加更多角色选择"
- "部分情况下情感标签与内容不太匹配"
- "希望前端能显示角色表情变化的动画"

#### 7.7.4、问题修复

根据用户反馈，我们进行了以下优化：
1. 优化对话历史渲染性能，引入虚拟列表（计划中）
2. 增加音频下载重试机制，提升稳定性
3. 优化LLM推理参数（temperature=0.7, top_p=0.9），提高情感标签准确率
4. 语音输入功能列入未来规划
5. Live2D表情动画列入未来规划

**用户满意度统计**（8位测试用户）：
- 非常满意：5人（62.5%）
- 比较满意：2人（25%）
- 一般：1人（12.5%）
- 不满意：0人

**整体评价**：用户对系统的角色扮演能力和语音质量评价很高，认为达到了"沉浸式体验"的目标。

---

## 8、项目成果与展望

### 8.1、项目成果总结

&emsp;&emsp;经过16周的开发，我们成功完成了"AI灵宠"虚拟桌宠系统的设计、开发、测试和部署。项目取得了以下成果：

#### 8.1.1、技术成果

1. **成功训练了两组高质量的角色模型**：
   - 基于6763条游戏原声训练的GPT-SoVITS模型，音色还原度>90%
   - 基于8000+对话样本微调的Qwen3-8B-QLoRA模型，角色风格准确度>85%

2. **构建了完整的数据工程流程**：
   - 从游戏资源解包到训练数据生成的全自动化流程
   - 创新性的情感标签映射方法，实现了多模态数据融合

3. **实现了高性能的模型服务化架构**：
   - 前后端分离 + 微服务设计
   - 4-bit量化技术使单张4090可同时运行4个模型服务
   - RESTful API设计规范，易于扩展和集成

4. **开发了功能丰富的前端应用**：
   - 支持多角色并发对话、自动转发、实时翻译
   - 完善的主题切换、国际化、对话历史管理功能
   - 优秀的用户体验和界面设计

#### 8.1.2、代码规模统计

**项目总代码量**：约15,000行

**语言分布**（基于GitHub统计）：
- Python: 96.0%（数据处理、模型训练、后端服务）
- Vue: 2.9%（前端页面组件）
- JavaScript: 0.8%（前端工具函数）
- TypeScript: 0.1%（前端类型定义）
- Shell: 0.1%（启动脚本）
- CSS: 0.1%（样式文件）

**详细代码统计**：

后端代码（Python）：
- LLM API服务：~800行（api.py核心逻辑）
- TTS API服务：~600行（重构后的api.py）
- 数据预处理脚本：~1,200行（textProcessor 1-5）
- 训练脚本：~400行（train4090.py）
- GPT-SoVITS框架代码：~8,000行（第三方框架）

前端代码（Vue/JavaScript）：
- Vue组件：~4,500行（ChatPage等核心组件）
- 状态管理：~800行（Pinia stores）
- API封装：~250行（models.js）
- 工具函数：~600行（翻译、i18n、请求等）
- 样式文件：~1,500行（CSS/SCSS）

配置文件和文档：
- Shell脚本：~200行（4个启动脚本）
- 配置文件：~300行（vite.config.js、package.json等）
- 文档（Markdown）：~5,500行（含本报告）

**文件统计**：
- Python文件：50+
- Vue/JavaScript文件：30+
- 配置文件：15+
- 文档文件：4个主要文档

**Git提交记录**：
- 总提交次数：9次（主分支）
- 开发周期：2024年9月 - 2024年12月
- 贡献者：2人（[@Soulw1nd](https://github.com/Soulw1nd), [@firefly163](https://github.com/firefly163)）
- 开源许可：MIT License

#### 8.1.3、项目文档

我们撰写了完整的项目文档，包括：
- **需求分析文档**：明确系统功能和性能需求
- **概要设计文档**：描述系统整体架构和模块划分
- **详细设计文档**：详细说明各模块的实现方法和关键技术
- **最终成果报告**：全面总结项目过程和成果
- **README文档**：提供快速上手指南和API说明

### 8.2、创新点与亮点

1. **情感化对话的创新实现**：

&emsp;&emsp;我们设计了一套情感标签系统：

**情感标签体系设计**：
```
<E:emotion_name>     # 情感标签
<M:modifier_name>    # 修饰标签（如闭眼、脸红等）
```

**支持的情感类型**（共21种）：
```
基本情感（8种）：
- smile     (微笑)          - serious   (严肃)
- thinking  (思考)          - worried   (担心)
- embarrassed (尴尬/害羞)   - angry     (生气)
- sad       (悲伤)          - shocked   (震惊)

进阶情感（5种）：
- surprised (惊讶)          - resigned  (无奈)
- smirk     (得意/坏笑)     - nervous   (紧张)
- confused  (困惑)          

修饰标签（2种）：
- eyes_closed (闭眼)        - blush     (脸红)
```

**情感标签的数据流**：
```
游戏立绘文件名 → 人工分析表情 → 映射为情感标签 → 
标注训练数据 → LLM学习情感-文本关联 → 
推理时生成情感标签 → 前端解析并展示（预留）
```

**技术难点与解决方案**：

| 难点 | 解决方案 |
|-|-|
| 如何从图片获取情感信息 | 人工分析约30张立绘（部分映射的文本过少未启用），建立映射表 |
| 如何让LLM学会情感标签 | 在训练数据中将情感标签放在回复的第一行，强制模型学习 |
| 如何保证标签的准确性 | 使用温度采样（temperature=0.7）平衡创造性和准确性 |
| 如何处理标签缺失情况 | 后端parse函数实现鲁棒的解析逻辑，缺失时使用默认值 |

**创新意义**：
- 将视觉情感信息融入语言模型训练
- 为虚拟角色的情感表达提供了可扩展的技术方案

1. **训练与推理的格式一致性保障**：
   - 复用原项目的chat_template配置
   - 确保训练数据生成、模型微调、API推理三个阶段的格式严格一致
   - 这是项目成功的关键经验之一

2. **云端部署的API重构**：
   - 重构了GPT-SoVITS的原版API，使其适合云端部署
   - 引入静态文件服务、线程锁、固定参考音频等机制
   - 显著提升了部署便利性和服务稳定性

3. **灵活的对话历史管理**：
   - LLM API支持三种调用模式（用户+历史、仅历史、仅用户）
   - 前端可灵活构建不同格式的历史记录
   - 支持最多24轮上下文，平衡了连贯性和性能

### 8.3、存在的不足

1. **语言限制**：
   - 当前模型本体仅支持日文，中英文用户需依赖前端搭载的翻译API
   - 未来可考虑训练多语言版本或使用多语言基座模型

2. **情感标签准确率**：
   - 极少数情况下情感标签不太合理
   - 可通过增加训练数据、调整训练策略进一步优化

3. **硬件门槛较高**：
   - 训练和部署都需要高性能GPU，普通用户难以本地运行
   - 未来可探索更激进的量化方案（如INT8、INT4）或模型蒸馏

4. **功能待完善**：
   - 前端表情切换功能未完全实现（已预留接口）
   - 缺少语音输入（ASR）功能
   - 对话导出、角色自定义等功能可进一步丰富

### 8.4、未来展望

#### 8.4.1、短期规划（1-3个月）

1. **完善现有功能**：
   - 实现前端角色表情动画（Live2D或Spine）
   - 增加语音输入功能（集成SenseVoice或Whisper）
   - 优化情感标签准确率

2. **新增角色**：
   - 基于相同技术路线，训练更多角色或支持社区平台模型
   - 构建角色库，用户可自由选择对话对象

3. **性能优化**：
   - 探索更高效的量化方案（GPTQ、AWQ）
   - 实现模型推理缓存，提升响应速度

#### 8.4.2、中期规划（3-6个月）

1. **多模态交互**：
   - 集成视觉模型（如Qwen-VL），支持图片理解
   - 支持用户上传图片，AI根据图片内容回复

2. **记忆系统**：
   - 引入长期记忆机制（如Letta框架）
   - AI能够记住用户的偏好、历史对话等信息

3. **移动端适配**：
   - 开发移动端Web应用或原生App
   - 优化界面和交互，适配小屏设备

#### 8.4.3、长期规划（6-12个月）

1. **开放平台化**：
   - 提供用户自训练角色的工具链
   - 支持用户上传自定义角色模型
   - 构建角色社区，用户可分享和下载角色

2. **商业化探索**：
   - 与游戏厂商合作，获取官方授权
   - 提供付费高级功能（如无限对话历史、专属角色等）
   - 探索2B市场（企业虚拟助手、客服机器人等）

3. **技术演进**：
   - 跟进最新的大语言模型技术（如GPT-5、Qwen3.5等）
   - 探索端侧部署方案（如MLC-LLM、llama.cpp）
   - 研究实时语音对话技术（如SoundStream + VAE）

---

## 9、团队分工与贡献率

### 9.1、团队成员

- **乐长昕**（学号：2023212455）
- **罗添元**（学号：2023212823）

### 9.2、详细分工

#### 9.2.1、罗添元的工作内容

**数据工程阶段**：
-  使用GARbro工具解包游戏资源
-  分析游戏脚本结构，设计数据提取方案
-  编写textProcessor1-5脚本（合并、统计、数据清洗、格式转换）
-  人工分析立绘图片，制定情感标签映射表
-  生成最终训练数据集（dataset.jsonl）

**模型训练阶段**：
- 配置云服务器训练环境
- 编写并调试QLoRA训练脚本（train4090.py）
- 执行Qwen3-8B模型微调训练
- GPT-SoVITS音频预处理（降噪、切分、ASR）和模型训练

**后端开发阶段**：
- 设计并实现LLM API核心逻辑（api.py）
- 重构GPT-SoVITS API，支持云端部署
- 编写启动脚本（run_api_llm.sh、run_api_sovits.sh）
- 云端部署和端口映射配置

**总计贡献率：50%**

#### 9.2.2、乐长昕的工作内容

**整体设计与测试检查**：
- 主催与项目进程监督
- 确定并设计整体技术路线
- 选择具体模型与训练方案
- 验证数据集质量和格式正确性
- LLM训练参数调优和效果验证
- TTS模型效果测试和优化
- 后端服务联调和bug修复
- 组织并邀请完成测试活动

**前端开发阶段**：
- 搭建Vue 3项目框架
- 开发ChatPage核心逻辑
- 实现对话历史管理和LocalStorage持久化
- 集成LLM和TTS API调用
- 实现翻译功能（百度API）
- 实现自动转发逻辑
- 开发主题切换和国际化功能
- UI/UX优化和bug修复

**文档撰写**：
- 撰写需求分析文档
- 撰写概要设计文档
- 撰写详细设计文档
- 撰写最终成果报告
- 整理项目README和使用文档

**总计贡献率：50%**

### 9.3、协作方式

1. **代码管理**：
   - 使用Git进行版本控制，代码托管在GitHub
   - 通过Pull Request进行代码审查
   - 保持main分支稳定，日常开发在dev分支进行

2. **任务分配**：
   - 每周例会确定本周任务目标
   - 使用GitHub Issues跟踪任务进度
   - 通过微信群及时沟通问题和进展

3. **文档协作**：
   - 使用腾讯文档实时协作编辑
   - 定期Review文档内容，确保准确性和一致性

4. **测试与部署**：
   - 共同参与系统测试和bug修复
   - 轮流负责云端服务器的维护和监控

### 9.4、贡献率总结

|成员|贡献率|主要贡献领域|
|-|-|-|
|**乐长昕**|**50%**|整体设计与测试检查、前端开发、文档撰写|
|**罗添元**|**50%**|数据工程、模型训练、后端开发|

&emsp;&emsp;两位成员在项目中的贡献是平等的，分工明确且协作顺畅。两人优势互补，共同完成了这个复杂的系统工程。

---

## 10、总结与致谢

### 10.1、项目总结

&emsp;&emsp;本项目从构思到完成历时16周，我们深入实践了大语言模型微调、语音合成、Web全栈开发、云端部署等多个技术领域，完成了一个功能完整、技术先进的虚拟桌宠系统。

&emsp;&emsp;通过这个项目，我们不仅掌握了QLoRA、GPT-SoVITS等前沿技术，更重要的是锻炼了**系统性思维**和**工程实践能力**：

- **数据工程思维**：从原始资源到训练数据的完整pipeline设计
- **模型工程能力**：模型训练、调优、部署的全流程实践
- **软件工程规范**：模块化设计、API规范、版本控制、文档撰写
- **问题解决能力**：面对各种技术难题，通过查阅文档、调试代码、社区求助等方式解决

&emsp;&emsp;这个项目让我们深刻体会到：**AI应用开发不仅是模型训练，更是一个涉及数据处理、模型服务化、前后端开发、用户体验设计的综合性工程**。

### 10.2、致谢

&emsp;&emsp;本项目的完成离不开以下各方的支持和帮助：

- **指导教师吴晓非老师**：感谢老师在项目过程中的悉心指导和建议答疑
- **annali07（luna-sama项目作者）**：感谢提供了宝贵的技术路线参考和灵感
- **RVC-Boss（GPT-SoVITS项目作者）**：感谢开源优秀的语音合成工具
- **Qwen团队**：感谢提供高质量的开源大语言模型
- **开源社区**：感谢Transformers、PEFT、FastAPI、Vue等开源项目的贡献者

&emsp;&emsp;最后，感谢我们的团队成员彼此的信任、支持和协作，让这个项目从想法变成现实！

---

**项目开源地址**：[https://github.com/firefly163/AI-Project](https://github.com/firefly163/AI-Project)

**中途开发仓库**：[https://github.com/Soulw1nd/magic_conch_frontend-master](https://github.com/Soulw1nd/magic_conch_frontend-master)（前端原型，由于含有百度翻译API Key已闭源）

**大文件下载**：由于GitHub单文件限制，模型权重等大文件（>100MB）托管在百度网盘：
- 链接：[https://pan.baidu.com/s/1OcJnSlo3SMSrUL3AsutuzA?pwd=rbrb](https://pan.baidu.com/s/1OcJnSlo3SMSrUL3AsutuzA?pwd=rbrb)
- 提取码：rbrb

**在线Demo**：云服务器已停机（成本考虑），需要演示时可重启。历史运行日志见`serverlog.txt`

**联系方式**：
- 乐长昕：2023212455@bupt.cn
- 罗添元：2023212823@bupt.cn

---

**文档撰写时间**：2025年12月

**最后更新时间**：2025年12月17日

